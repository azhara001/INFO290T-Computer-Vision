<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>INFO 290-T Final Project Report</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
  body {
    padding: 2cm; 
  }
}
</style>


</head>

<body>

<h1 id="toc_0">INFO 290-T Final Project</h1>

<h2 id="toc_1">Facial Recogntion using Fisher Faces -vs- Eigen Faces using Support Vector Machines</h2>

<h3 id="toc_2">Abdullah Azhar</h3>

<hr>

<h5 id="toc_3">Datasets Used:</h5>

<table>
<thead>
<tr>
<th style="text-align: left">Sr. No.</th>
<th style="text-align: left">Dataset</th>
<th style="text-align: center">Source</th>
<th style="text-align: right">#Images</th>
<th style="text-align: right">#Classes</th>
<th style="text-align: right">#Features (per image)</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: left">1.</td>
<td style="text-align: left">Olivetti Dataset</td>
<td style="text-align: center"><a href="http://web.math.ucsb.edu/%7Eatzberg/pmwiki_intranet/index.php?n=AtzbergerHomePage.Notes1?setskin=atzbergerLayout1">Olivetti Dataset (.npy)</a></td>
<td style="text-align: right">400</td>
<td style="text-align: right">40</td>
<td style="text-align: right">4096 (64x64)</td>
</tr>
<tr>
<td style="text-align: left">2.</td>
<td style="text-align: left">Labelled Faces in the Wild</td>
<td style="text-align: center">Sklearn  (fetch_LFW _sklearn)</td>
<td style="text-align: right"><em>user defined</em></td>
<td style="text-align: right"><em>user defined</em></td>
<td style="text-align: right">1850 (50x37)</td>
</tr>
<tr>
<td style="text-align: left">3.</td>
<td style="text-align: left">Labelled Faced in the Wild (unprocessed)</td>
<td style="text-align: center"><a href="http://vis-www.cs.umass.edu/lfw/">LFW_Dataset</a></td>
<td style="text-align: right"><em>user defined</em></td>
<td style="text-align: right"><em>user defined</em></td>
<td style="text-align: right">62500 (250x250)</td>
</tr>
</tbody>
</table>

<p><br></p>

<hr>

<h5 id="toc_4">Project Abstract Overview:</h5>

<p><strong>The overarching goal of this project was to build a face recognition algorithm that classifies faces using labelled data (supervised learning only)</strong> <br></p>

<p>For this purpose, two feature extraction methods were used:<br>
    1. Eigenfaces (Turk, M., &amp; Pentland, A. (1991). Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1), 71â€“86.)<br>
    2. Fisherfaces (Belhumeur, P.N., Hespanha, J.P. and Kriegman, D. (1997) Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19, 711-720.
http://dx.doi.org/10.1109/34.598228) <br></p>

<p><strong>Note: The code for eigenfaces and fisherfaces was written inhouse i.e., no external library was used for this purpose</strong> <br></p>

<p>Next, a support vector machine (using sklearn) was trained with the following kernels: <br>
1. Linear <br>
2. Radial Basis Function <br></p>

<p>To test and exhaust the classification algorithm, three different datasets with increasing complexity were used (as mentioned above). As well, the number of eigenfaces were also tested and the results were evaluated and compared <br></p>

<p>It is worthwhile to mention that a series configuration is used here such that the dataset is fed into eigenfaces, the output of which is fed into fisherfaces who&#39;s output is finally fed into an SVM. (one may think of this as iteratively reducing the dimensionality of the dataset while retaining the maximum information about the dataset while simultaneously separating out the between class variance and minimizing the within class variance</p>

<h2 id="toc_5"><br></h2>

<h5 id="toc_6">Exploratory Data Analysis:</h5>

<h4 id="toc_7">1. Olivetti Dataset</h4>

<h5 id="toc_8">1.1 Sample of Images</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Sample%20Olivetti.png" alt="alt text"></p>

<p>10 Images per Class (Total of 40 Classes) </p>

<h5 id="toc_9">1.2 TSNE Visualization</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/Olivetti%20DatasetTSNE.png" alt="alt text"></p>

<h5 id="toc_10">1.3 Mean Across all faces</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/Olivetti%20DatasetMeans.png" alt="alt text"></p>

<h5 id="toc_11">1.4 First 12 Canonical Basis (Eigen-Faces)</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/EigenFaces_Olivetti.png" alt="alt text"></p>

<p><strong>Once the canonical basis were retrieved, selection of the number of basis to be used were based on two factors:<br><br></strong>
    1. Either on the percentage of fall-out rate i.e., the sum of eigen values as a percentage over the total sum of eigen values. For this purpose, a value of 88% fall-off was used which reduced the dimensionality from 4096x400 to 53x400<br><br>
    2. To compute fisher faces, a problem as mentioned by Peter N. Belhumeur, Joao P Hespanha, David J. Kriegman, 1997/7, is getting a within class scatter matrix as non-singular (rank at most can be N-c) where N is the number of images and c is the number of classes. To avoid this problem, one may either introduce noise into the within class matrix to avoid the rank deficieny or one may use PCA to reduce the dimensionality to N-c followed by computing fisher faces to further reduce the dimensinality to c-1. </p>

<h5 id="toc_12">1.5 Classification Using Support Vector Machines (Linear -vs- Non-linear)</h5>

<p>Once the dataset has been zero-meaned and projected onto it&#39;s canonical basis, the next step is to build a classification machinery using support vector machines. For this purpose, sklearn was used:<br></p>

<p><code>X_train, X_test, y_train, y_test = train_test_split(dataset.T, labels, test_size=0.30, random_state=42)</code>
<code>param_grid = {
            &#39;C&#39;: [1e3, 5e3, 1e4, 5e4, 1e5],
            &#39;gamma&#39;: [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1],
            }
</code>
<code>clf = GridSearchCV(SVC(kernel=&#39;linear&#39;, class_weight=&#39;balanced&#39;), param_grid) # linear SVM!)</code>
<code>clf = clf.fit(X_train, y_train)</code>
<code>y_pred = clf.predict(X_test)</code>
<code>Accuracy_Score = accuracy_score(y_test, y_pred)</code>
<code>Classification_Report = classification_report(y_test, y_pred)</code>
<code>Confusion_Matrix = confusion_matrix(y_test, y_pred)</code></p>

<p>1.5.1 <strong>Linear SVM Classification</strong> 
 Initially, a linear SVM was trained to predict the performance of the model on the dataset.</p>

<p>This resulted in the following confusion matrix:</p>

<table>
<thead>
<tr>
<th>Class</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>1</td>
<td>0.2</td>
<td>0.333333333</td>
<td>5</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>5</td>
<td>0.75</td>
<td>1</td>
<td>0.857142857</td>
<td>3</td>
</tr>
<tr>
<td>6</td>
<td>1</td>
<td>0.333333333</td>
<td>0.5</td>
<td>3</td>
</tr>
<tr>
<td>7</td>
<td>1</td>
<td>0.875</td>
<td>0.933333333</td>
<td>8</td>
</tr>
<tr>
<td>8</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>9</td>
<td>0.75</td>
<td>1</td>
<td>0.857142857</td>
<td>3</td>
</tr>
<tr>
<td>10</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>11</td>
<td>1</td>
<td>0.8</td>
<td>0.888888889</td>
<td>5</td>
</tr>
<tr>
<td>12</td>
<td>0.666666667</td>
<td>1</td>
<td>0.8</td>
<td>2</td>
</tr>
<tr>
<td>13</td>
<td>0.75</td>
<td>1</td>
<td>0.857142857</td>
<td>3</td>
</tr>
<tr>
<td>14</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>15</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>16</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>17</td>
<td>0.666666667</td>
<td>0.666666667</td>
<td>0.666666667</td>
<td>3</td>
</tr>
<tr>
<td>18</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>19</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>20</td>
<td>0.666666667</td>
<td>1</td>
<td>0.8</td>
<td>2</td>
</tr>
<tr>
<td>21</td>
<td>0.5</td>
<td>1</td>
<td>0.666666667</td>
<td>1</td>
</tr>
<tr>
<td>22</td>
<td>0.8</td>
<td>1</td>
<td>0.888888889</td>
<td>4</td>
</tr>
<tr>
<td>23</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>24</td>
<td>1</td>
<td>0.666666667</td>
<td>0.8</td>
<td>3</td>
</tr>
<tr>
<td>25</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>26</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>27</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>28</td>
<td>1</td>
<td>0.5</td>
<td>0.666666667</td>
<td>4</td>
</tr>
<tr>
<td>29</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>30</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>31</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>32</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>33</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>34</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
<td>2</td>
</tr>
<tr>
<td>35</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>36</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>37</td>
<td>0.75</td>
<td>1</td>
<td>0.857142857</td>
<td>3</td>
</tr>
<tr>
<td>38</td>
<td>0.75</td>
<td>0.857142857</td>
<td>0.8</td>
<td>7</td>
</tr>
<tr>
<td>39</td>
<td>0.8</td>
<td>1</td>
<td>0.888888889</td>
<td>4</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>accuracy</td>
<td>0.883333333</td>
<td>0.883333333</td>
<td>0.883333333</td>
<td>0.883333333</td>
</tr>
<tr>
<td>macro avg</td>
<td>0.88375</td>
<td>0.884970238</td>
<td>0.864047619</td>
<td>120</td>
</tr>
<tr>
<td>weighted avg</td>
<td>0.915138889</td>
<td>0.883333333</td>
<td>0.875066138</td>
<td>120</td>
</tr>
</tbody>
</table>

<p><br></p>

<p>1.5.2 <strong>Non Linear SVM Classification</strong>
The only change made to the above mentioned SVM was in the GridSearchCV function
<code>clf = GridSearchCV(SVC(kernel=&#39;rbf&#39;, class_weight=&#39;balanced&#39;), param_grid) # non-linear SVM (Radial Basis Function Kernel!)</code></p>

<p>The Classification Report for a non-linear SVM is as follows:</p>

<table>
<thead>
<tr>
<th>Class</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>1.000000</td>
<td>0.200000</td>
<td>0.333333</td>
<td>5.000000</td>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>0.750000</td>
<td>0.857143</td>
<td>4.000000</td>
</tr>
<tr>
<td>2</td>
<td>0.333333</td>
<td>0.500000</td>
<td>0.400000</td>
<td>2.000000</td>
</tr>
<tr>
<td>3</td>
<td>1.000000</td>
<td>0.250000</td>
<td>0.400000</td>
<td>4.000000</td>
</tr>
<tr>
<td>4</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>5</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>6</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>7</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>8.000000</td>
</tr>
<tr>
<td>8</td>
<td>0.666667</td>
<td>1.000000</td>
<td>0.800000</td>
<td>2.000000</td>
</tr>
<tr>
<td>9</td>
<td>1.000000</td>
<td>0.666667</td>
<td>0.800000</td>
<td>3.000000</td>
</tr>
<tr>
<td>10</td>
<td>1.000000</td>
<td>0.666667</td>
<td>0.800000</td>
<td>3.000000</td>
</tr>
<tr>
<td>11</td>
<td>1.000000</td>
<td>0.200000</td>
<td>0.333333</td>
<td>5.000000</td>
</tr>
<tr>
<td>12</td>
<td>0.047619</td>
<td>1.000000</td>
<td>0.090909</td>
<td>2.000000</td>
</tr>
<tr>
<td>13</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>14</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>15</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>16</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>17</td>
<td>1.000000</td>
<td>0.666667</td>
<td>0.800000</td>
<td>3.000000</td>
</tr>
<tr>
<td>18</td>
<td>1.000000</td>
<td>0.500000</td>
<td>0.666667</td>
<td>2.000000</td>
</tr>
<tr>
<td>19</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>20</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>21</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>22</td>
<td>1.000000</td>
<td>0.250000</td>
<td>0.400000</td>
<td>4.000000</td>
</tr>
<tr>
<td>23</td>
<td>1.000000</td>
<td>0.750000</td>
<td>0.857143</td>
<td>4.000000</td>
</tr>
<tr>
<td>24</td>
<td>1.000000</td>
<td>0.666667</td>
<td>0.800000</td>
<td>3.000000</td>
</tr>
<tr>
<td>25</td>
<td>0.333333</td>
<td>1.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>26</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>27</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>28</td>
<td>1.000000</td>
<td>0.500000</td>
<td>0.666667</td>
<td>4.000000</td>
</tr>
<tr>
<td>29</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>30</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>31</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>32</td>
<td>1.000000</td>
<td>0.666667</td>
<td>0.800000</td>
<td>3.000000</td>
</tr>
<tr>
<td>33</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>34</td>
<td>1.000000</td>
<td>0.500000</td>
<td>0.666667</td>
<td>2.000000</td>
</tr>
<tr>
<td>35</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>36</td>
<td>1.000000</td>
<td>0.500000</td>
<td>0.666667</td>
<td>2.000000</td>
</tr>
<tr>
<td>37</td>
<td>1.000000</td>
<td>0.666667</td>
<td>0.800000</td>
<td>3.000000</td>
</tr>
<tr>
<td>38</td>
<td>1.000000</td>
<td>0.285714</td>
<td>0.444444</td>
<td>7.000000</td>
</tr>
<tr>
<td>39</td>
<td>1.000000</td>
<td>0.500000</td>
<td>0.666667</td>
<td>4.000000</td>
</tr>
<tr>
<td>accuracy</td>
<td>0.591667</td>
<td>0.591667</td>
<td>0.591667</td>
<td>0.591667</td>
</tr>
<tr>
<td>macroavg</td>
<td>0.884524</td>
<td>0.667143</td>
<td>0.701241</td>
<td>120.000000</td>
</tr>
<tr>
<td>weightedavg</td>
<td>0.889683</td>
<td>0.591667</td>
<td>0.652639</td>
<td>120.000000</td>
</tr>
</tbody>
</table>

<p>For the two classification techniques i.e., linear versus radial basis function support vector machines, it turns out that for this dataset, the linear SVM performs better ~88% accuracy versus the radial basis function which has an accuracy of ~59%</p>

<h5 id="toc_13">1.6 Computation of Fisher Faces Canonical Basis</h5>

<p>The computation of Fisher Faces takes the reduced dimensionality of the Eigen Faces, and further reduces the features from N-c to c-1</p>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/Canonical%20Basis%20Fisher%20Faces%20Olivetti.png" alt="alt text"></p>

<h5 id="toc_14">1.7 Classification Using Support Vector Machines (Linear -vs- Non-linear) for Fisher Faces <br></h5>

<p><strong>Linear SVM Classification</strong> </p>

<p>The Classification Report for a non-linear SVM is as follows:</p>

<table>
<thead>
<tr>
<th>Class</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>5</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>6</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>7</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>8</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>9</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>10</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>11</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>12</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>13</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>14</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>15</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>17</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>18</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>19</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>20</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>21</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>22</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>23</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>24</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>25</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>26</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>27</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>28</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>29</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>30</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>31</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>32</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>33</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>34</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>35</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>36</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>37</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>38</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>39</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>accuracy</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>macro avg</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>120</td>
</tr>
<tr>
<td>weighted avg</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>120</td>
</tr>
</tbody>
</table>

<p>From the above classification report, the 100% accuracy seems too idealistic. One may argue that the dataset chosen is extremely ideal for real-world face recognition applications. To mitigate this confusion, two subsequent steps were taken: <br>
1. Rotating one third of the images in the dataset by 180 degrees to check whether the accuracy changes <br>
2. Moving on to a more complex dataset</p>

<h5 id="toc_15">1.8 Manipulating Olivetti Dataset Images to confuse the SVM Classifier</h5>

<p>New Eigen-Faces: (having one third of the images flipped)
<img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/Flipped%20EigenFaces_Olivetti.png" alt="alt text"></p>

<p>Classification Report For Eigen-Faces:</p>

<table>
<thead>
<tr>
<th>Classes</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>0.5</td>
<td>0.4</td>
<td>0.444444444</td>
<td>5</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>2</td>
<td>0.2</td>
<td>0.5</td>
<td>0.285714286</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>0.5</td>
<td>0.666666667</td>
<td>4</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>0.666666667</td>
<td>0.8</td>
<td>3</td>
</tr>
<tr>
<td>5</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>6</td>
<td>1</td>
<td>0.666666667</td>
<td>0.8</td>
<td>3</td>
</tr>
<tr>
<td>7</td>
<td>1</td>
<td>0.625</td>
<td>0.769230769</td>
<td>8</td>
</tr>
<tr>
<td>8</td>
<td>1</td>
<td>0.5</td>
<td>0.666666667</td>
<td>2</td>
</tr>
<tr>
<td>9</td>
<td>0.5</td>
<td>0.333333333</td>
<td>0.4</td>
<td>3</td>
</tr>
<tr>
<td>10</td>
<td>1</td>
<td>0.666666667</td>
<td>0.8</td>
<td>3</td>
</tr>
<tr>
<td>11</td>
<td>1</td>
<td>0.8</td>
<td>0.888888889</td>
<td>5</td>
</tr>
<tr>
<td>12</td>
<td>1</td>
<td>0.5</td>
<td>0.666666667</td>
<td>2</td>
</tr>
<tr>
<td>13</td>
<td>1</td>
<td>0.666666667</td>
<td>0.8</td>
<td>3</td>
</tr>
<tr>
<td>14</td>
<td>0.75</td>
<td>1</td>
<td>0.857142857</td>
<td>3</td>
</tr>
<tr>
<td>15</td>
<td>0.5</td>
<td>0.666666667</td>
<td>0.571428571</td>
<td>3</td>
</tr>
<tr>
<td>16</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>17</td>
<td>0.5</td>
<td>0.333333333</td>
<td>0.4</td>
<td>3</td>
</tr>
<tr>
<td>18</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
<td>2</td>
</tr>
<tr>
<td>19</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>20</td>
<td>0.4</td>
<td>1</td>
<td>0.571428571</td>
<td>2</td>
</tr>
<tr>
<td>21</td>
<td>0.5</td>
<td>1</td>
<td>0.666666667</td>
<td>1</td>
</tr>
<tr>
<td>22</td>
<td>0.75</td>
<td>0.75</td>
<td>0.75</td>
<td>4</td>
</tr>
<tr>
<td>23</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>24</td>
<td>0.5</td>
<td>1</td>
<td>0.666666667</td>
<td>3</td>
</tr>
<tr>
<td>25</td>
<td>0.4</td>
<td>1</td>
<td>0.571428571</td>
<td>2</td>
</tr>
<tr>
<td>26</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>27</td>
<td>1</td>
<td>0.666666667</td>
<td>0.8</td>
<td>3</td>
</tr>
<tr>
<td>28</td>
<td>0.5</td>
<td>0.75</td>
<td>0.6</td>
<td>4</td>
</tr>
<tr>
<td>29</td>
<td>1</td>
<td>0.666666667</td>
<td>0.8</td>
<td>3</td>
</tr>
<tr>
<td>30</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>31</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>32</td>
<td>1</td>
<td>0.666666667</td>
<td>0.8</td>
<td>3</td>
</tr>
<tr>
<td>33</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>34</td>
<td>0.333333333</td>
<td>0.5</td>
<td>0.4</td>
<td>2</td>
</tr>
<tr>
<td>35</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>36</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>37</td>
<td>0.75</td>
<td>1</td>
<td>0.857142857</td>
<td>3</td>
</tr>
<tr>
<td>38</td>
<td>0.8</td>
<td>0.571428571</td>
<td>0.666666667</td>
<td>7</td>
</tr>
<tr>
<td>39</td>
<td>1</td>
<td>0.75</td>
<td>0.857142857</td>
<td>4</td>
</tr>
<tr>
<td>accuracy</td>
<td>0.725</td>
<td>0.725</td>
<td>0.725</td>
<td>0.725</td>
</tr>
<tr>
<td>macro avg</td>
<td>0.759583333</td>
<td>0.716160714</td>
<td>0.708099817</td>
<td>120</td>
</tr>
<tr>
<td>weighted avg</td>
<td>0.814722222</td>
<td>0.725</td>
<td>0.741916972</td>
<td>120</td>
</tr>
</tbody>
</table>

<p><strong>Flipping every third image has indeed reduced the accuracy on classification based on eigen-faces from ~88% to ~71%</strong></p>

<p>New Fisher-Faces:
<img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/Flipped%20Canonical%20Basis%20Fisher%20Faces%20Olivetti.png" alt="alt text"></p>

<p>Classification Report for Fisher Faces:</p>

<table>
<thead>
<tr>
<th>Classes</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>5</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>6</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>7</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>8</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>9</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>10</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>11</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>12</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>13</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>14</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>15</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>17</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>18</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>19</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>20</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>21</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>22</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>23</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>24</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>25</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>26</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>27</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>28</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>29</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>30</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>31</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>32</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>33</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>34</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>35</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>36</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>37</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>38</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>39</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>accuracy</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>macro avg</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>120</td>
</tr>
<tr>
<td>weighted avg</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>120</td>
</tr>
</tbody>
</table>

<p><strong>Turns out that Classification Accuracy for the Fisher Faces is still unchanged for this case. One reason may as well be that fisher faces is able to minimize the within class variance and maximize the inter class variance for the flipped images as well. A possible way to check this would be to flip all the test set data before checking for the accuracy</strong></p>

<p>Intuitively, our model should churn out an extremely low accuracy score:</p>

<p>Classification Report for Fisher Faces (test data flipped)</p>

<table>
<thead>
<tr>
<th>Classes</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>4</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>5</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>6</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>7</td>
<td>0.25</td>
<td>0.25</td>
<td>0.25</td>
<td>8</td>
</tr>
<tr>
<td>8</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>9</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>10</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>11</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5</td>
</tr>
<tr>
<td>12</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>13</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>14</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>15</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>17</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>18</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>19</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>20</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>21</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>22</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>23</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>24</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>25</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>26</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>27</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>28</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>29</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>30</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>31</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>32</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>33</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>34</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>35</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>36</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>37</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>38</td>
<td>0.285714286</td>
<td>0.285714286</td>
<td>0.285714286</td>
<td>7</td>
</tr>
<tr>
<td>39</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>accuracy</td>
<td>0.033333333</td>
<td>0.033333333</td>
<td>0.033333333</td>
<td>0.033333333</td>
</tr>
<tr>
<td>macro avg</td>
<td>0.013736264</td>
<td>0.013736264</td>
<td>0.013736264</td>
<td>120</td>
</tr>
<tr>
<td>weighted avg</td>
<td>0.033333333</td>
<td>0.033333333</td>
<td>0.033333333</td>
<td>120</td>
</tr>
</tbody>
</table>

<p><strong>While the ambiguity behind the fisher scores attained earlier is still there, the previous example corroborates our understanding of not getting a good accuracy score which means that the model predictions are atleast making intuitive sense. To resolve this ambiguity, the next step would be to change the dataset and then observe the same metrics</strong></p>

<p><strong>As well, for now we were directly computing the eigenfaces and the fisher faces from the original dataset! Going forward, we will be feeding the eigenfaces (with reduced dimensionality) into computing fisherfaces before we pass that onto our SVM classifier. Here we will play around with the number of eigenvectors used for fisher analysis and notice the effect of this number on our classifiers</strong></p>

<h4 id="toc_16">2. Labelled Faces in the Wild Sklearn (12 Classes)</h4>

<p>Since the Labelled Faces in the Wild (LFW) dataset contains ~13,000 images, sklearn provides us with a chance to extract a subset of this dataset. 
<br>
For this first case, we set the minimum number of images per class to be: 50 <br>
This results in the following dataset: <br>
Number of Images: 1560 <br>
Number of Classes: 12 <br>
Features per image: 1850 (50x37) so a low resolution imagery is retrieved! </p>

<h5 id="toc_17">2.1 Sample Images</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/LFW_Sklearn%20sample%20images.png" alt="alt text"></p>

<h5 id="toc_18">2.2 TSNE Visualization</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/LFW_sklearnTSNE.png" alt="alt text"></p>

<p>TSNE Visuzalization for this dataset shows that classes are not as spread out (atleast in the 2-D projection) so it will be interesting to note how our classifier performs</p>

<h5 id="toc_19">2.3 Mean across all faces</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/LFW%20SklearnMeans.png" alt="alt text"></p>

<h5 id="toc_20">2.4 First 12 Canonical Basis (Eigen-Faces)</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/EigenFaces_Olivetti.png" alt=""></p>

<h5 id="toc_21">2.5 Classification Using Support Vector Machines (Linear)</h5>

<table>
<thead>
<tr>
<th>Classes</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>0.382352941</td>
<td>0.619047619</td>
<td>0.472727273</td>
<td>21</td>
</tr>
<tr>
<td>1</td>
<td>0.696202532</td>
<td>0.714285714</td>
<td>0.705128205</td>
<td>77</td>
</tr>
<tr>
<td>2</td>
<td>0.325</td>
<td>0.40625</td>
<td>0.361111111</td>
<td>32</td>
</tr>
<tr>
<td>3</td>
<td>0.813253012</td>
<td>0.833333333</td>
<td>0.823170732</td>
<td>162</td>
</tr>
<tr>
<td>4</td>
<td>0.548387097</td>
<td>0.566666667</td>
<td>0.557377049</td>
<td>30</td>
</tr>
<tr>
<td>5</td>
<td>0.6</td>
<td>0.571428571</td>
<td>0.585365854</td>
<td>21</td>
</tr>
<tr>
<td>6</td>
<td>0.666666667</td>
<td>0.235294118</td>
<td>0.347826087</td>
<td>17</td>
</tr>
<tr>
<td>7</td>
<td>0.666666667</td>
<td>0.588235294</td>
<td>0.625</td>
<td>17</td>
</tr>
<tr>
<td>8</td>
<td>0.888888889</td>
<td>0.421052632</td>
<td>0.571428571</td>
<td>19</td>
</tr>
<tr>
<td>9</td>
<td>0.833333333</td>
<td>0.769230769</td>
<td>0.8</td>
<td>13</td>
</tr>
<tr>
<td>10</td>
<td>0.666666667</td>
<td>0.615384615</td>
<td>0.64</td>
<td>13</td>
</tr>
<tr>
<td>11</td>
<td>0.613636364</td>
<td>0.586956522</td>
<td>0.6</td>
<td>46</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>accuracy</td>
<td>0.666666667</td>
<td>0.666666667</td>
<td>0.666666667</td>
<td>0.666666667</td>
</tr>
<tr>
<td>macro avg</td>
<td>0.641754514</td>
<td>0.577263821</td>
<td>0.59076124</td>
<td>468</td>
</tr>
<tr>
<td>weighted avg</td>
<td>0.684013752</td>
<td>0.666666667</td>
<td>0.666368674</td>
<td>468</td>
</tr>
</tbody>
</table>

<h5 id="toc_22">2.6 Computation of Fisher Faces Canonical Basis</h5>

<p>First 12 Canonical Basis for LFW Dataset imported from sklearn</p>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/Canonical%20Basis%20Fisher%20Faces%20LFW_sklearn.png" alt="alt text"></p>

<h5 id="toc_23">2.7 Classification Using Support Vector Machines (Linear) for Fisher Faces <br></h5>

<table>
<thead>
<tr>
<th>Classes</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>21</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>77</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>32</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>162</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>30</td>
</tr>
<tr>
<td>5</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>21</td>
</tr>
<tr>
<td>6</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>17</td>
</tr>
<tr>
<td>7</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>17</td>
</tr>
<tr>
<td>8</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>19</td>
</tr>
<tr>
<td>9</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>13</td>
</tr>
<tr>
<td>10</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>13</td>
</tr>
<tr>
<td>11</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>46</td>
</tr>
<tr>
<td>accuracy</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>macro avg</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>468</td>
</tr>
<tr>
<td>weighted avg</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>468</td>
</tr>
</tbody>
</table>

<p><strong>The next step here would be to increase the number of classes to see how the classifier performs</strong></p>

<h4 id="toc_24">3. Labelled Faces in the Wild Sklearn (156 Classes)</h4>

<h5 id="toc_25">Adding more classes to break the fisherfaces classification</h5>

<p>For the second case, we set the minimum number of images per class to be: 10 <br>
This results in the following dataset: <br>
Number of Images: 4309 <br>
Number of Classes: 156 <br>
Features per image: 1850 (50x37) so a low resolution imagery is retrieved! </p>

<h5 id="toc_26">3.1 Sample Images</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/LFW_Sklearn%20sample%20images.png" alt="alt text"></p>

<h5 id="toc_27">3.2 TSNE Visualization</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/LFW_sklearn_156TSNE.png" alt="alt text"></p>

<h5 id="toc_28">3.3 Mean across all faces</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/LFW%20SklearnMeans_156_classes.png" alt="alt text"></p>

<h5 id="toc_29">3.4 First 12 Canonical Basis (Eigen-Faces)</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/EigenFaces_LFW_sklearn_156.png" alt="alt text"></p>

<h5 id="toc_30">3.5.1 Classification Using Support Vector Machines (Linear) at 88% fall-off</h5>

<table>
<thead>
<tr>
<th>Class</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>7.000000</td>
</tr>
<tr>
<td>1</td>
<td>0.200000</td>
<td>0.500000</td>
<td>0.285714</td>
<td>2.000000</td>
</tr>
<tr>
<td>2</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>8.000000</td>
</tr>
<tr>
<td>3</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>13.000000</td>
</tr>
<tr>
<td>4</td>
<td>0.100000</td>
<td>0.142857</td>
<td>0.117647</td>
<td>7.000000</td>
</tr>
<tr>
<td>5</td>
<td>0.222222</td>
<td>0.153846</td>
<td>0.181818</td>
<td>13.000000</td>
</tr>
<tr>
<td>6</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>7</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>8</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>9</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>10</td>
<td>0.166667</td>
<td>0.142857</td>
<td>0.153846</td>
<td>7.000000</td>
</tr>
<tr>
<td>11</td>
<td>0.257143</td>
<td>0.375000</td>
<td>0.305085</td>
<td>24.000000</td>
</tr>
<tr>
<td>12</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>10.000000</td>
</tr>
<tr>
<td>13</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>14</td>
<td>0.120000</td>
<td>0.300000</td>
<td>0.171429</td>
<td>10.000000</td>
</tr>
<tr>
<td>15</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>16</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>17</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>18</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>19</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>20</td>
<td>0.125000</td>
<td>0.200000</td>
<td>0.153846</td>
<td>5.000000</td>
</tr>
<tr>
<td>21</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>22</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>23</td>
<td>0.417582</td>
<td>0.457831</td>
<td>0.436782</td>
<td>83.000000</td>
</tr>
<tr>
<td>24</td>
<td>0.250000</td>
<td>0.333333</td>
<td>0.285714</td>
<td>3.000000</td>
</tr>
<tr>
<td>25</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>26</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>27</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>28</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>29</td>
<td>0.266667</td>
<td>0.375000</td>
<td>0.311688</td>
<td>32.000000</td>
</tr>
<tr>
<td>30</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>31</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>32</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>9.000000</td>
</tr>
<tr>
<td>33</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>34</td>
<td>0.142857</td>
<td>0.166667</td>
<td>0.153846</td>
<td>6.000000</td>
</tr>
<tr>
<td>35</td>
<td>0.413636</td>
<td>0.514124</td>
<td>0.458438</td>
<td>177.000000</td>
</tr>
<tr>
<td>36</td>
<td>0.214286</td>
<td>0.281250</td>
<td>0.243243</td>
<td>32.000000</td>
</tr>
<tr>
<td>37</td>
<td>0.312500</td>
<td>0.312500</td>
<td>0.312500</td>
<td>16.000000</td>
</tr>
<tr>
<td>38</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>39</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>40</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>41</td>
<td>0.307692</td>
<td>0.363636</td>
<td>0.333333</td>
<td>11.000000</td>
</tr>
<tr>
<td>42</td>
<td>0.181818</td>
<td>0.400000</td>
<td>0.250000</td>
<td>5.000000</td>
</tr>
<tr>
<td>43</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>8.000000</td>
</tr>
<tr>
<td>44</td>
<td>0.375000</td>
<td>0.187500</td>
<td>0.250000</td>
<td>16.000000</td>
</tr>
<tr>
<td>45</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>7.000000</td>
</tr>
<tr>
<td>46</td>
<td>0.200000</td>
<td>0.125000</td>
<td>0.153846</td>
<td>8.000000</td>
</tr>
<tr>
<td>47</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>48</td>
<td>0.200000</td>
<td>0.166667</td>
<td>0.181818</td>
<td>6.000000</td>
</tr>
<tr>
<td>49</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>19.000000</td>
</tr>
<tr>
<td>50</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>51</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>52</td>
<td>0.200000</td>
<td>0.090909</td>
<td>0.125000</td>
<td>11.000000</td>
</tr>
<tr>
<td>53</td>
<td>0.333333</td>
<td>0.333333</td>
<td>0.333333</td>
<td>3.000000</td>
</tr>
<tr>
<td>54</td>
<td>0.142857</td>
<td>0.142857</td>
<td>0.142857</td>
<td>14.000000</td>
</tr>
<tr>
<td>55</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>56</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>57</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>58</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>59</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>60</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>61</td>
<td>0.368421</td>
<td>0.388889</td>
<td>0.378378</td>
<td>18.000000</td>
</tr>
<tr>
<td>62</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>63</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>64</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>9.000000</td>
</tr>
<tr>
<td>65</td>
<td>0.157895</td>
<td>0.166667</td>
<td>0.162162</td>
<td>18.000000</td>
</tr>
<tr>
<td>66</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>67</td>
<td>0.142857</td>
<td>0.142857</td>
<td>0.142857</td>
<td>7.000000</td>
</tr>
<tr>
<td>68</td>
<td>0.166667</td>
<td>0.111111</td>
<td>0.133333</td>
<td>9.000000</td>
</tr>
<tr>
<td>69</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>71</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>7.000000</td>
</tr>
<tr>
<td>72</td>
<td>0.428571</td>
<td>0.500000</td>
<td>0.461538</td>
<td>6.000000</td>
</tr>
<tr>
<td>73</td>
<td>0.250000</td>
<td>0.181818</td>
<td>0.210526</td>
<td>11.000000</td>
</tr>
<tr>
<td>74</td>
<td>0.200000</td>
<td>0.125000</td>
<td>0.153846</td>
<td>8.000000</td>
</tr>
<tr>
<td>75</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>8.000000</td>
</tr>
<tr>
<td>76</td>
<td>0.200000</td>
<td>0.250000</td>
<td>0.222222</td>
<td>4.000000</td>
</tr>
<tr>
<td>77</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>12.000000</td>
</tr>
<tr>
<td>78</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>79</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>80</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>81</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>9.000000</td>
</tr>
<tr>
<td>82</td>
<td>0.142857</td>
<td>0.142857</td>
<td>0.142857</td>
<td>7.000000</td>
</tr>
<tr>
<td>83</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>84</td>
<td>0.250000</td>
<td>0.166667</td>
<td>0.200000</td>
<td>6.000000</td>
</tr>
<tr>
<td>85</td>
<td>0.333333</td>
<td>0.272727</td>
<td>0.300000</td>
<td>11.000000</td>
</tr>
<tr>
<td>86</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>87</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>88</td>
<td>1.000000</td>
<td>0.200000</td>
<td>0.333333</td>
<td>5.000000</td>
</tr>
<tr>
<td>89</td>
<td>0.111111</td>
<td>0.333333</td>
<td>0.166667</td>
<td>3.000000</td>
</tr>
<tr>
<td>90</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>9.000000</td>
</tr>
<tr>
<td>91</td>
<td>0.333333</td>
<td>0.181818</td>
<td>0.235294</td>
<td>11.000000</td>
</tr>
<tr>
<td>92</td>
<td>0.166667</td>
<td>0.200000</td>
<td>0.181818</td>
<td>5.000000</td>
</tr>
<tr>
<td>93</td>
<td>0.375000</td>
<td>0.187500</td>
<td>0.250000</td>
<td>16.000000</td>
</tr>
<tr>
<td>94</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>95</td>
<td>0.181818</td>
<td>0.166667</td>
<td>0.173913</td>
<td>12.000000</td>
</tr>
<tr>
<td>96</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>97</td>
<td>0.200000</td>
<td>0.500000</td>
<td>0.285714</td>
<td>4.000000</td>
</tr>
<tr>
<td>98</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>99</td>
<td>0.428571</td>
<td>0.545455</td>
<td>0.480000</td>
<td>11.000000</td>
</tr>
<tr>
<td>100</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>8.000000</td>
</tr>
<tr>
<td>101</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>102</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>103</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>104</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>105</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>106</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>107</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>108</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>9.000000</td>
</tr>
<tr>
<td>109</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>10.000000</td>
</tr>
<tr>
<td>110</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>111</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>112</td>
<td>0.200000</td>
<td>0.200000</td>
<td>0.200000</td>
<td>5.000000</td>
</tr>
<tr>
<td>113</td>
<td>0.166667</td>
<td>0.500000</td>
<td>0.250000</td>
<td>2.000000</td>
</tr>
<tr>
<td>114</td>
<td>0.333333</td>
<td>0.200000</td>
<td>0.250000</td>
<td>5.000000</td>
</tr>
<tr>
<td>115</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>116</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>117</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>118</td>
<td>0.090909</td>
<td>0.200000</td>
<td>0.125000</td>
<td>5.000000</td>
</tr>
<tr>
<td>119</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>120</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>121</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>9.000000</td>
</tr>
<tr>
<td>122</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>123</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>10.000000</td>
</tr>
<tr>
<td>124</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>126</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>127</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>128</td>
<td>0.166667</td>
<td>0.142857</td>
<td>0.153846</td>
<td>7.000000</td>
</tr>
<tr>
<td>129</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>130</td>
<td>0.333333</td>
<td>0.166667</td>
<td>0.222222</td>
<td>6.000000</td>
</tr>
<tr>
<td>131</td>
<td>0.200000</td>
<td>0.166667</td>
<td>0.181818</td>
<td>6.000000</td>
</tr>
<tr>
<td>132</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>133</td>
<td>0.428571</td>
<td>0.400000</td>
<td>0.413793</td>
<td>15.000000</td>
</tr>
<tr>
<td>134</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>7.000000</td>
</tr>
<tr>
<td>135</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>136</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>13.000000</td>
</tr>
<tr>
<td>137</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>138</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>139</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>140</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>141</td>
<td>0.100000</td>
<td>0.200000</td>
<td>0.133333</td>
<td>5.000000</td>
</tr>
<tr>
<td>142</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>143</td>
<td>0.111111</td>
<td>0.200000</td>
<td>0.142857</td>
<td>5.000000</td>
</tr>
<tr>
<td>144</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>145</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>9.000000</td>
</tr>
<tr>
<td>146</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>9.000000</td>
</tr>
<tr>
<td>147</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>148</td>
<td>0.272727</td>
<td>0.300000</td>
<td>0.285714</td>
<td>30.000000</td>
</tr>
<tr>
<td>149</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>150</td>
<td>0.200000</td>
<td>0.200000</td>
<td>0.200000</td>
<td>5.000000</td>
</tr>
<tr>
<td>151</td>
<td>0.250000</td>
<td>0.090909</td>
<td>0.133333</td>
<td>11.000000</td>
</tr>
<tr>
<td>152</td>
<td>0.181818</td>
<td>0.133333</td>
<td>0.153846</td>
<td>15.000000</td>
</tr>
<tr>
<td>153</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>154</td>
<td>0.142857</td>
<td>0.250000</td>
<td>0.181818</td>
<td>4.000000</td>
</tr>
<tr>
<td>155</td>
<td>0.250000</td>
<td>0.125000</td>
<td>0.166667</td>
<td>8.000000</td>
</tr>
<tr>
<td>156</td>
<td>0.142857</td>
<td>0.333333</td>
<td>0.200000</td>
<td>3.000000</td>
</tr>
<tr>
<td>accuracy</td>
<td>0.203403</td>
<td>0.203403</td>
<td>0.203403</td>
<td>0.203403</td>
</tr>
<tr>
<td>macroavg</td>
<td>0.090751</td>
<td>0.093829</td>
<td>0.085580</td>
<td>1293.000000</td>
</tr>
<tr>
<td>weightedavg</td>
<td>0.188455</td>
<td>0.203403</td>
<td>0.189843</td>
<td>1293.000000</td>
</tr>
</tbody>
</table>

<h5 id="toc_31">3.5.2 Classification Using Support Vector Machines (Linear) at 88% fall-off (fisher-faces)</h5>

<table>
<thead>
<tr>
<th>Class</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>7.000000</td>
</tr>
<tr>
<td>1</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>2</td>
<td>0.230769</td>
<td>0.375000</td>
<td>0.285714</td>
<td>8.000000</td>
</tr>
<tr>
<td>3</td>
<td>0.200000</td>
<td>0.153846</td>
<td>0.173913</td>
<td>13.000000</td>
</tr>
<tr>
<td>4</td>
<td>0.200000</td>
<td>0.285714</td>
<td>0.235294</td>
<td>7.000000</td>
</tr>
<tr>
<td>5</td>
<td>0.062500</td>
<td>0.076923</td>
<td>0.068966</td>
<td>13.000000</td>
</tr>
<tr>
<td>6</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>7</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>8</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>9</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>10</td>
<td>1.000000</td>
<td>0.428571</td>
<td>0.600000</td>
<td>7.000000</td>
</tr>
<tr>
<td>11</td>
<td>0.413793</td>
<td>0.500000</td>
<td>0.452830</td>
<td>24.000000</td>
</tr>
<tr>
<td>12</td>
<td>0.076923</td>
<td>0.100000</td>
<td>0.086957</td>
<td>10.000000</td>
</tr>
<tr>
<td>13</td>
<td>0.100000</td>
<td>0.250000</td>
<td>0.142857</td>
<td>4.000000</td>
</tr>
<tr>
<td>14</td>
<td>0.157895</td>
<td>0.300000</td>
<td>0.206897</td>
<td>10.000000</td>
</tr>
<tr>
<td>15</td>
<td>0.250000</td>
<td>0.250000</td>
<td>0.250000</td>
<td>4.000000</td>
</tr>
<tr>
<td>16</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>17</td>
<td>0.142857</td>
<td>0.333333</td>
<td>0.200000</td>
<td>3.000000</td>
</tr>
<tr>
<td>18</td>
<td>0.250000</td>
<td>0.250000</td>
<td>0.250000</td>
<td>4.000000</td>
</tr>
<tr>
<td>19</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>20</td>
<td>0.200000</td>
<td>0.200000</td>
<td>0.200000</td>
<td>5.000000</td>
</tr>
<tr>
<td>21</td>
<td>0.142857</td>
<td>0.333333</td>
<td>0.200000</td>
<td>3.000000</td>
</tr>
<tr>
<td>22</td>
<td>0.500000</td>
<td>0.200000</td>
<td>0.285714</td>
<td>5.000000</td>
</tr>
<tr>
<td>23</td>
<td>0.487179</td>
<td>0.457831</td>
<td>0.472050</td>
<td>83.000000</td>
</tr>
<tr>
<td>24</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>25</td>
<td>0.058824</td>
<td>0.250000</td>
<td>0.095238</td>
<td>4.000000</td>
</tr>
<tr>
<td>26</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>27</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>28</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>29</td>
<td>0.451613</td>
<td>0.437500</td>
<td>0.444444</td>
<td>32.000000</td>
</tr>
<tr>
<td>30</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>31</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>32</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>9.000000</td>
</tr>
<tr>
<td>33</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>34</td>
<td>0.111111</td>
<td>0.166667</td>
<td>0.133333</td>
<td>6.000000</td>
</tr>
<tr>
<td>35</td>
<td>0.741379</td>
<td>0.485876</td>
<td>0.587031</td>
<td>177.000000</td>
</tr>
<tr>
<td>36</td>
<td>0.365854</td>
<td>0.468750</td>
<td>0.410959</td>
<td>32.000000</td>
</tr>
<tr>
<td>37</td>
<td>0.769231</td>
<td>0.625000</td>
<td>0.689655</td>
<td>16.000000</td>
</tr>
<tr>
<td>38</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>39</td>
<td>0.500000</td>
<td>0.500000</td>
<td>0.500000</td>
<td>4.000000</td>
</tr>
<tr>
<td>40</td>
<td>0.076923</td>
<td>0.166667</td>
<td>0.105263</td>
<td>6.000000</td>
</tr>
<tr>
<td>41</td>
<td>0.538462</td>
<td>0.636364</td>
<td>0.583333</td>
<td>11.000000</td>
</tr>
<tr>
<td>42</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>43</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>8.000000</td>
</tr>
<tr>
<td>44</td>
<td>0.500000</td>
<td>0.437500</td>
<td>0.466667</td>
<td>16.000000</td>
</tr>
<tr>
<td>45</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>7.000000</td>
</tr>
<tr>
<td>46</td>
<td>0.500000</td>
<td>0.125000</td>
<td>0.200000</td>
<td>8.000000</td>
</tr>
<tr>
<td>47</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>48</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>49</td>
<td>0.095238</td>
<td>0.105263</td>
<td>0.100000</td>
<td>19.000000</td>
</tr>
<tr>
<td>50</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>51</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>52</td>
<td>0.333333</td>
<td>0.181818</td>
<td>0.235294</td>
<td>11.000000</td>
</tr>
<tr>
<td>53</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>54</td>
<td>0.190476</td>
<td>0.285714</td>
<td>0.228571</td>
<td>14.000000</td>
</tr>
<tr>
<td>55</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>56</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>57</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>58</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>59</td>
<td>0.600000</td>
<td>0.600000</td>
<td>0.600000</td>
<td>5.000000</td>
</tr>
<tr>
<td>60</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>61</td>
<td>0.333333</td>
<td>0.277778</td>
<td>0.303030</td>
<td>18.000000</td>
</tr>
<tr>
<td>62</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>63</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>64</td>
<td>0.250000</td>
<td>0.111111</td>
<td>0.153846</td>
<td>9.000000</td>
</tr>
<tr>
<td>65</td>
<td>0.100000</td>
<td>0.055556</td>
<td>0.071429</td>
<td>18.000000</td>
</tr>
<tr>
<td>66</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>67</td>
<td>0.400000</td>
<td>0.285714</td>
<td>0.333333</td>
<td>7.000000</td>
</tr>
<tr>
<td>68</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>9.000000</td>
</tr>
<tr>
<td>69</td>
<td>0.166667</td>
<td>0.200000</td>
<td>0.181818</td>
<td>5.000000</td>
</tr>
<tr>
<td>70</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>71</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>7.000000</td>
</tr>
<tr>
<td>72</td>
<td>0.500000</td>
<td>0.166667</td>
<td>0.250000</td>
<td>6.000000</td>
</tr>
<tr>
<td>73</td>
<td>0.312500</td>
<td>0.454545</td>
<td>0.370370</td>
<td>11.000000</td>
</tr>
<tr>
<td>74</td>
<td>1.000000</td>
<td>0.375000</td>
<td>0.545455</td>
<td>8.000000</td>
</tr>
<tr>
<td>75</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>8.000000</td>
</tr>
<tr>
<td>76</td>
<td>0.500000</td>
<td>0.500000</td>
<td>0.500000</td>
<td>4.000000</td>
</tr>
<tr>
<td>77</td>
<td>0.333333</td>
<td>0.166667</td>
<td>0.222222</td>
<td>12.000000</td>
</tr>
<tr>
<td>78</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>79</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>80</td>
<td>0.200000</td>
<td>0.166667</td>
<td>0.181818</td>
<td>6.000000</td>
</tr>
<tr>
<td>81</td>
<td>0.428571</td>
<td>0.333333</td>
<td>0.375000</td>
<td>9.000000</td>
</tr>
<tr>
<td>82</td>
<td>0.285714</td>
<td>0.571429</td>
<td>0.380952</td>
<td>7.000000</td>
</tr>
<tr>
<td>83</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>84</td>
<td>0.666667</td>
<td>0.666667</td>
<td>0.666667</td>
<td>6.000000</td>
</tr>
<tr>
<td>85</td>
<td>0.538462</td>
<td>0.636364</td>
<td>0.583333</td>
<td>11.000000</td>
</tr>
<tr>
<td>86</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>87</td>
<td>0.300000</td>
<td>0.600000</td>
<td>0.400000</td>
<td>5.000000</td>
</tr>
<tr>
<td>88</td>
<td>0.500000</td>
<td>0.200000</td>
<td>0.285714</td>
<td>5.000000</td>
</tr>
<tr>
<td>89</td>
<td>0.166667</td>
<td>0.333333</td>
<td>0.222222</td>
<td>3.000000</td>
</tr>
<tr>
<td>90</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>9.000000</td>
</tr>
<tr>
<td>91</td>
<td>0.333333</td>
<td>0.545455</td>
<td>0.413793</td>
<td>11.000000</td>
</tr>
<tr>
<td>92</td>
<td>0.142857</td>
<td>0.200000</td>
<td>0.166667</td>
<td>5.000000</td>
</tr>
<tr>
<td>93</td>
<td>0.615385</td>
<td>0.500000</td>
<td>0.551724</td>
<td>16.000000</td>
</tr>
<tr>
<td>94</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>95</td>
<td>0.500000</td>
<td>0.416667</td>
<td>0.454545</td>
<td>12.000000</td>
</tr>
<tr>
<td>96</td>
<td>0.166667</td>
<td>0.333333</td>
<td>0.222222</td>
<td>3.000000</td>
</tr>
<tr>
<td>97</td>
<td>0.111111</td>
<td>0.250000</td>
<td>0.153846</td>
<td>4.000000</td>
</tr>
<tr>
<td>98</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>99</td>
<td>0.600000</td>
<td>0.545455</td>
<td>0.571429</td>
<td>11.000000</td>
</tr>
<tr>
<td>100</td>
<td>0.200000</td>
<td>0.125000</td>
<td>0.153846</td>
<td>8.000000</td>
</tr>
<tr>
<td>101</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>102</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>103</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>104</td>
<td>0.333333</td>
<td>0.333333</td>
<td>0.333333</td>
<td>3.000000</td>
</tr>
<tr>
<td>105</td>
<td>0.142857</td>
<td>0.166667</td>
<td>0.153846</td>
<td>6.000000</td>
</tr>
<tr>
<td>106</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>107</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>108</td>
<td>0.333333</td>
<td>0.222222</td>
<td>0.266667</td>
<td>9.000000</td>
</tr>
<tr>
<td>109</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>10.000000</td>
</tr>
<tr>
<td>110</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>111</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>112</td>
<td>0.333333</td>
<td>0.400000</td>
<td>0.363636</td>
<td>5.000000</td>
</tr>
<tr>
<td>113</td>
<td>0.200000</td>
<td>0.500000</td>
<td>0.285714</td>
<td>2.000000</td>
</tr>
<tr>
<td>114</td>
<td>0.166667</td>
<td>0.200000</td>
<td>0.181818</td>
<td>5.000000</td>
</tr>
<tr>
<td>115</td>
<td>0.200000</td>
<td>0.250000</td>
<td>0.222222</td>
<td>4.000000</td>
</tr>
<tr>
<td>116</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>117</td>
<td>0.500000</td>
<td>0.600000</td>
<td>0.545455</td>
<td>5.000000</td>
</tr>
<tr>
<td>118</td>
<td>0.222222</td>
<td>0.400000</td>
<td>0.285714</td>
<td>5.000000</td>
</tr>
<tr>
<td>119</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>120</td>
<td>0.200000</td>
<td>0.250000</td>
<td>0.222222</td>
<td>4.000000</td>
</tr>
<tr>
<td>121</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>9.000000</td>
</tr>
<tr>
<td>122</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>123</td>
<td>0.222222</td>
<td>0.200000</td>
<td>0.210526</td>
<td>10.000000</td>
</tr>
<tr>
<td>124</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>126</td>
<td>0.090909</td>
<td>0.166667</td>
<td>0.117647</td>
<td>6.000000</td>
</tr>
<tr>
<td>127</td>
<td>0.500000</td>
<td>0.200000</td>
<td>0.285714</td>
<td>5.000000</td>
</tr>
<tr>
<td>128</td>
<td>0.500000</td>
<td>0.428571</td>
<td>0.461538</td>
<td>7.000000</td>
</tr>
<tr>
<td>129</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>130</td>
<td>0.083333</td>
<td>0.166667</td>
<td>0.111111</td>
<td>6.000000</td>
</tr>
<tr>
<td>131</td>
<td>0.428571</td>
<td>0.500000</td>
<td>0.461538</td>
<td>6.000000</td>
</tr>
<tr>
<td>132</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>133</td>
<td>0.538462</td>
<td>0.466667</td>
<td>0.500000</td>
<td>15.000000</td>
</tr>
<tr>
<td>134</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>7.000000</td>
</tr>
<tr>
<td>135</td>
<td>0.500000</td>
<td>0.333333</td>
<td>0.400000</td>
<td>3.000000</td>
</tr>
<tr>
<td>136</td>
<td>0.250000</td>
<td>0.153846</td>
<td>0.190476</td>
<td>13.000000</td>
</tr>
<tr>
<td>137</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>138</td>
<td>0.250000</td>
<td>0.200000</td>
<td>0.222222</td>
<td>5.000000</td>
</tr>
<tr>
<td>139</td>
<td>0.250000</td>
<td>0.250000</td>
<td>0.250000</td>
<td>4.000000</td>
</tr>
<tr>
<td>140</td>
<td>0.100000</td>
<td>0.200000</td>
<td>0.133333</td>
<td>5.000000</td>
</tr>
<tr>
<td>141</td>
<td>0.500000</td>
<td>0.200000</td>
<td>0.285714</td>
<td>5.000000</td>
</tr>
<tr>
<td>142</td>
<td>0.666667</td>
<td>0.400000</td>
<td>0.500000</td>
<td>5.000000</td>
</tr>
<tr>
<td>143</td>
<td>0.375000</td>
<td>0.600000</td>
<td>0.461538</td>
<td>5.000000</td>
</tr>
<tr>
<td>144</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>145</td>
<td>0.181818</td>
<td>0.222222</td>
<td>0.200000</td>
<td>9.000000</td>
</tr>
<tr>
<td>146</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>9.000000</td>
</tr>
<tr>
<td>147</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>148</td>
<td>0.192308</td>
<td>0.166667</td>
<td>0.178571</td>
<td>30.000000</td>
</tr>
<tr>
<td>149</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>150</td>
<td>0.333333</td>
<td>0.400000</td>
<td>0.363636</td>
<td>5.000000</td>
</tr>
<tr>
<td>151</td>
<td>0.250000</td>
<td>0.181818</td>
<td>0.210526</td>
<td>11.000000</td>
</tr>
<tr>
<td>152</td>
<td>0.142857</td>
<td>0.133333</td>
<td>0.137931</td>
<td>15.000000</td>
</tr>
<tr>
<td>153</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>154</td>
<td>0.500000</td>
<td>0.250000</td>
<td>0.333333</td>
<td>4.000000</td>
</tr>
<tr>
<td>155</td>
<td>0.428571</td>
<td>0.375000</td>
<td>0.400000</td>
<td>8.000000</td>
</tr>
<tr>
<td>156</td>
<td>1.000000</td>
<td>0.666667</td>
<td>0.800000</td>
<td>3.000000</td>
</tr>
<tr>
<td>accuracy</td>
<td>0.286156</td>
<td>0.286156</td>
<td>0.286156</td>
<td>0.286156</td>
</tr>
<tr>
<td>macroavg</td>
<td>0.209008</td>
<td>0.190024</td>
<td>0.186511</td>
<td>1293.000000</td>
</tr>
<tr>
<td>weightedavg</td>
<td>0.337749</td>
<td>0.286156</td>
<td>0.299149</td>
<td>1293.000000</td>
</tr>
</tbody>
</table>

<p><strong>While using eigenfaces at 88% fall off did end up in 156 classes that resulted in poor accuracy scores for both the eigenfaces classification as well as the fisherfaces classification, it is worth noting that fisherfaces may have performed poorly because the dimensions computed were less than c-1</strong></p>

<p><strong>To verify this assumption, eigenfaces with reduced dimensionality of N-c were fed into the fisher faces and the results are as follows</strong></p>

<h5 id="toc_32">3.5.3 Classification Using Support Vector Machines (Linear) with N-c classes (eigen-faces)</h5>

<table>
<thead>
<tr>
<th>Classes</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>7</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>0.181818182</td>
<td>0.25</td>
<td>0.210526316</td>
<td>8</td>
</tr>
<tr>
<td>3</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>13</td>
</tr>
<tr>
<td>4</td>
<td>0.142857143</td>
<td>0.142857143</td>
<td>0.142857143</td>
<td>7</td>
</tr>
<tr>
<td>5</td>
<td>0.230769231</td>
<td>0.230769231</td>
<td>0.230769231</td>
<td>13</td>
</tr>
<tr>
<td>6</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5</td>
</tr>
<tr>
<td>7</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>8</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>9</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>10</td>
<td>0.5</td>
<td>0.142857143</td>
<td>0.222222222</td>
<td>7</td>
</tr>
<tr>
<td>11</td>
<td>0.375</td>
<td>0.5</td>
<td>0.428571429</td>
<td>24</td>
</tr>
<tr>
<td>12</td>
<td>0.05</td>
<td>0.1</td>
<td>0.066666667</td>
<td>10</td>
</tr>
<tr>
<td>13</td>
<td>0.052631579</td>
<td>0.25</td>
<td>0.086956522</td>
<td>4</td>
</tr>
<tr>
<td>14</td>
<td>0.142857143</td>
<td>0.2</td>
<td>0.166666667</td>
<td>10</td>
</tr>
<tr>
<td>15</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>16</td>
<td>0.5</td>
<td>0.25</td>
<td>0.333333333</td>
<td>4</td>
</tr>
<tr>
<td>17</td>
<td>0.333333333</td>
<td>0.333333333</td>
<td>0.333333333</td>
<td>3</td>
</tr>
<tr>
<td>18</td>
<td>0.5</td>
<td>0.25</td>
<td>0.333333333</td>
<td>4</td>
</tr>
<tr>
<td>19</td>
<td>0.142857143</td>
<td>1</td>
<td>0.25</td>
<td>1</td>
</tr>
<tr>
<td>20</td>
<td>0.5</td>
<td>0.2</td>
<td>0.285714286</td>
<td>5</td>
</tr>
<tr>
<td>21</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>22</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5</td>
</tr>
<tr>
<td>23</td>
<td>0.565656566</td>
<td>0.674698795</td>
<td>0.615384615</td>
<td>83</td>
</tr>
<tr>
<td>24</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>25</td>
<td>0.055555556</td>
<td>0.25</td>
<td>0.090909091</td>
<td>4</td>
</tr>
<tr>
<td>26</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>6</td>
</tr>
<tr>
<td>27</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>28</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>29</td>
<td>0.425</td>
<td>0.53125</td>
<td>0.472222222</td>
<td>32</td>
</tr>
<tr>
<td>30</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>6</td>
</tr>
<tr>
<td>31</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>32</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>9</td>
</tr>
<tr>
<td>33</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>34</td>
<td>0.666666667</td>
<td>0.333333333</td>
<td>0.444444444</td>
<td>6</td>
</tr>
<tr>
<td>35</td>
<td>0.442307692</td>
<td>0.779661017</td>
<td>0.564417178</td>
<td>177</td>
</tr>
<tr>
<td>36</td>
<td>0.338709677</td>
<td>0.65625</td>
<td>0.446808511</td>
<td>32</td>
</tr>
<tr>
<td>37</td>
<td>0.454545455</td>
<td>0.3125</td>
<td>0.37037037</td>
<td>16</td>
</tr>
<tr>
<td>38</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>39</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>40</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>6</td>
</tr>
<tr>
<td>41</td>
<td>0.333333333</td>
<td>0.454545455</td>
<td>0.384615385</td>
<td>11</td>
</tr>
<tr>
<td>42</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5</td>
</tr>
<tr>
<td>43</td>
<td>1</td>
<td>0.125</td>
<td>0.222222222</td>
<td>8</td>
</tr>
<tr>
<td>44</td>
<td>0.555555556</td>
<td>0.3125</td>
<td>0.4</td>
<td>16</td>
</tr>
<tr>
<td>45</td>
<td>1</td>
<td>0.142857143</td>
<td>0.25</td>
<td>7</td>
</tr>
<tr>
<td>46</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>8</td>
</tr>
<tr>
<td>47</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>48</td>
<td>0.25</td>
<td>0.166666667</td>
<td>0.2</td>
<td>6</td>
</tr>
<tr>
<td>49</td>
<td>0.227272727</td>
<td>0.263157895</td>
<td>0.243902439</td>
<td>19</td>
</tr>
<tr>
<td>50</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>51</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>6</td>
</tr>
<tr>
<td>52</td>
<td>0.4</td>
<td>0.181818182</td>
<td>0.25</td>
<td>11</td>
</tr>
<tr>
<td>53</td>
<td>0.333333333</td>
<td>0.333333333</td>
<td>0.333333333</td>
<td>3</td>
</tr>
<tr>
<td>54</td>
<td>0.214285714</td>
<td>0.214285714</td>
<td>0.214285714</td>
<td>14</td>
</tr>
<tr>
<td>55</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>56</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>57</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>58</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>59</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5</td>
</tr>
<tr>
<td>60</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>61</td>
<td>0.37037037</td>
<td>0.555555556</td>
<td>0.444444444</td>
<td>18</td>
</tr>
<tr>
<td>62</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>63</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>64</td>
<td>1</td>
<td>0.333333333</td>
<td>0.5</td>
<td>9</td>
</tr>
<tr>
<td>65</td>
<td>0.25</td>
<td>0.166666667</td>
<td>0.2</td>
<td>18</td>
</tr>
<tr>
<td>66</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>67</td>
<td>0.166666667</td>
<td>0.142857143</td>
<td>0.153846154</td>
<td>7</td>
</tr>
<tr>
<td>68</td>
<td>0.666666667</td>
<td>0.222222222</td>
<td>0.333333333</td>
<td>9</td>
</tr>
<tr>
<td>69</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5</td>
</tr>
<tr>
<td>71</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>7</td>
</tr>
<tr>
<td>72</td>
<td>0.666666667</td>
<td>0.333333333</td>
<td>0.444444444</td>
<td>6</td>
</tr>
<tr>
<td>73</td>
<td>0.266666667</td>
<td>0.363636364</td>
<td>0.307692308</td>
<td>11</td>
</tr>
<tr>
<td>74</td>
<td>1</td>
<td>0.25</td>
<td>0.4</td>
<td>8</td>
</tr>
<tr>
<td>75</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>8</td>
</tr>
<tr>
<td>76</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>77</td>
<td>0.5</td>
<td>0.25</td>
<td>0.333333333</td>
<td>12</td>
</tr>
<tr>
<td>78</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>79</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5</td>
</tr>
<tr>
<td>80</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>6</td>
</tr>
<tr>
<td>81</td>
<td>0.4</td>
<td>0.222222222</td>
<td>0.285714286</td>
<td>9</td>
</tr>
<tr>
<td>82</td>
<td>0.4</td>
<td>0.285714286</td>
<td>0.333333333</td>
<td>7</td>
</tr>
<tr>
<td>83</td>
<td>0.4</td>
<td>0.4</td>
<td>0.4</td>
<td>5</td>
</tr>
<tr>
<td>84</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
<td>6</td>
</tr>
<tr>
<td>85</td>
<td>0.307692308</td>
<td>0.363636364</td>
<td>0.333333333</td>
<td>11</td>
</tr>
<tr>
<td>86</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>87</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5</td>
</tr>
<tr>
<td>88</td>
<td>0.5</td>
<td>0.2</td>
<td>0.285714286</td>
<td>5</td>
</tr>
<tr>
<td>89</td>
<td>0.2</td>
<td>0.333333333</td>
<td>0.25</td>
<td>3</td>
</tr>
<tr>
<td>90</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>9</td>
</tr>
<tr>
<td>91</td>
<td>0.538461538</td>
<td>0.636363636</td>
<td>0.583333333</td>
<td>11</td>
</tr>
<tr>
<td>92</td>
<td>0.1</td>
<td>0.2</td>
<td>0.133333333</td>
<td>5</td>
</tr>
<tr>
<td>93</td>
<td>0.727272727</td>
<td>0.5</td>
<td>0.592592593</td>
<td>16</td>
</tr>
<tr>
<td>94</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>95</td>
<td>0.5</td>
<td>0.25</td>
<td>0.333333333</td>
<td>12</td>
</tr>
<tr>
<td>96</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>97</td>
<td>0.6</td>
<td>0.75</td>
<td>0.666666667</td>
<td>4</td>
</tr>
<tr>
<td>98</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>99</td>
<td>0.571428571</td>
<td>0.727272727</td>
<td>0.64</td>
<td>11</td>
</tr>
<tr>
<td>100</td>
<td>0.142857143</td>
<td>0.125</td>
<td>0.133333333</td>
<td>8</td>
</tr>
<tr>
<td>101</td>
<td>0.666666667</td>
<td>0.666666667</td>
<td>0.666666667</td>
<td>3</td>
</tr>
<tr>
<td>102</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>103</td>
<td>0.333333333</td>
<td>0.333333333</td>
<td>0.333333333</td>
<td>3</td>
</tr>
<tr>
<td>104</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>105</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>6</td>
</tr>
<tr>
<td>106</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>107</td>
<td>0.333333333</td>
<td>1</td>
<td>0.5</td>
<td>1</td>
</tr>
<tr>
<td>108</td>
<td>0.166666667</td>
<td>0.111111111</td>
<td>0.133333333</td>
<td>9</td>
</tr>
<tr>
<td>109</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>10</td>
</tr>
<tr>
<td>110</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>111</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>112</td>
<td>0.5</td>
<td>0.2</td>
<td>0.285714286</td>
<td>5</td>
</tr>
<tr>
<td>113</td>
<td>0.142857143</td>
<td>0.5</td>
<td>0.222222222</td>
<td>2</td>
</tr>
<tr>
<td>114</td>
<td>1</td>
<td>0.2</td>
<td>0.333333333</td>
<td>5</td>
</tr>
<tr>
<td>115</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>116</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>117</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5</td>
</tr>
<tr>
<td>118</td>
<td>0.125</td>
<td>0.2</td>
<td>0.153846154</td>
<td>5</td>
</tr>
<tr>
<td>119</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>120</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>121</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>9</td>
</tr>
<tr>
<td>122</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>6</td>
</tr>
<tr>
<td>123</td>
<td>0.166666667</td>
<td>0.1</td>
<td>0.125</td>
<td>10</td>
</tr>
<tr>
<td>124</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>125</td>
<td>0.25</td>
<td>0.5</td>
<td>0.333333333</td>
<td>2</td>
</tr>
<tr>
<td>126</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>6</td>
</tr>
<tr>
<td>127</td>
<td>0.5</td>
<td>0.2</td>
<td>0.285714286</td>
<td>5</td>
</tr>
<tr>
<td>128</td>
<td>0.428571429</td>
<td>0.428571429</td>
<td>0.428571429</td>
<td>7</td>
</tr>
<tr>
<td>129</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>130</td>
<td>0.333333333</td>
<td>0.166666667</td>
<td>0.222222222</td>
<td>6</td>
</tr>
<tr>
<td>131</td>
<td>0.428571429</td>
<td>0.5</td>
<td>0.461538462</td>
<td>6</td>
</tr>
<tr>
<td>132</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>133</td>
<td>0.666666667</td>
<td>0.533333333</td>
<td>0.592592593</td>
<td>15</td>
</tr>
<tr>
<td>134</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>7</td>
</tr>
<tr>
<td>135</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>136</td>
<td>1</td>
<td>0.076923077</td>
<td>0.142857143</td>
<td>13</td>
</tr>
<tr>
<td>137</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>138</td>
<td>1</td>
<td>0.2</td>
<td>0.333333333</td>
<td>5</td>
</tr>
<tr>
<td>139</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>140</td>
<td>0.25</td>
<td>0.2</td>
<td>0.222222222</td>
<td>5</td>
</tr>
<tr>
<td>141</td>
<td>0.125</td>
<td>0.2</td>
<td>0.153846154</td>
<td>5</td>
</tr>
<tr>
<td>142</td>
<td>1</td>
<td>0.2</td>
<td>0.333333333</td>
<td>5</td>
</tr>
<tr>
<td>143</td>
<td>0.333333333</td>
<td>0.4</td>
<td>0.363636364</td>
<td>5</td>
</tr>
<tr>
<td>144</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>145</td>
<td>0.166666667</td>
<td>0.111111111</td>
<td>0.133333333</td>
<td>9</td>
</tr>
<tr>
<td>146</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>9</td>
</tr>
<tr>
<td>147</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>148</td>
<td>0.25</td>
<td>0.466666667</td>
<td>0.325581395</td>
<td>30</td>
</tr>
<tr>
<td>149</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>150</td>
<td>0.285714286</td>
<td>0.4</td>
<td>0.333333333</td>
<td>5</td>
</tr>
<tr>
<td>151</td>
<td>0.285714286</td>
<td>0.181818182</td>
<td>0.222222222</td>
<td>11</td>
</tr>
<tr>
<td>152</td>
<td>0.090909091</td>
<td>0.066666667</td>
<td>0.076923077</td>
<td>15</td>
</tr>
<tr>
<td>153</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>154</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>4</td>
</tr>
<tr>
<td>155</td>
<td>0.4</td>
<td>0.25</td>
<td>0.307692308</td>
<td>8</td>
</tr>
<tr>
<td>156</td>
<td>0.666666667</td>
<td>0.666666667</td>
<td>0.666666667</td>
<td>3</td>
</tr>
<tr>
<td>accuracy</td>
<td>0.329466357</td>
<td>0.329466357</td>
<td>0.329466357</td>
<td>0.329466357</td>
</tr>
<tr>
<td>macro avg</td>
<td>0.215286964</td>
<td>0.171489465</td>
<td>0.165859297</td>
<td>1293</td>
</tr>
<tr>
<td>weighted avg</td>
<td>0.319319429</td>
<td>0.329466357</td>
<td>0.293954712</td>
<td>1293</td>
</tr>
</tbody>
</table>

<h5 id="toc_33">3.5.4 Classification Using Support Vector Machines (Linear) with c-1 classes (fisher-faces)</h5>

<table>
<thead>
<tr>
<th>Classes</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>13</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>0.857142857</td>
<td>0.923076923</td>
<td>7</td>
</tr>
<tr>
<td>5</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>13</td>
</tr>
<tr>
<td>6</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>7</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>8</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>9</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>10</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>11</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>24</td>
</tr>
<tr>
<td>12</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>13</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>14</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>15</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>16</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>17</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>18</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>19</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>20</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>21</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>22</td>
<td>1</td>
<td>0.8</td>
<td>0.888888889</td>
<td>5</td>
</tr>
<tr>
<td>23</td>
<td>0.988095238</td>
<td>1</td>
<td>0.994011976</td>
<td>83</td>
</tr>
<tr>
<td>24</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>25</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>26</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>27</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>28</td>
<td>1</td>
<td>0.75</td>
<td>0.857142857</td>
<td>4</td>
</tr>
<tr>
<td>29</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>32</td>
</tr>
<tr>
<td>30</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>31</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>32</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>9</td>
</tr>
<tr>
<td>33</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>34</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>35</td>
<td>0.967213115</td>
<td>1</td>
<td>0.983333333</td>
<td>177</td>
</tr>
<tr>
<td>36</td>
<td>0.96969697</td>
<td>1</td>
<td>0.984615385</td>
<td>32</td>
</tr>
<tr>
<td>37</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>16</td>
</tr>
<tr>
<td>38</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>39</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>40</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>41</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>11</td>
</tr>
<tr>
<td>42</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>43</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>44</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>16</td>
</tr>
<tr>
<td>45</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>46</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>47</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>48</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>49</td>
<td>1</td>
<td>0.947368421</td>
<td>0.972972973</td>
<td>19</td>
</tr>
<tr>
<td>50</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>51</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>52</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>11</td>
</tr>
<tr>
<td>53</td>
<td>1</td>
<td>0.666666667</td>
<td>0.8</td>
<td>3</td>
</tr>
<tr>
<td>54</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>14</td>
</tr>
<tr>
<td>56</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>57</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>58</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>59</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>60</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>61</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>18</td>
</tr>
<tr>
<td>62</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>63</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>64</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>9</td>
</tr>
<tr>
<td>65</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>18</td>
</tr>
<tr>
<td>66</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>67</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>68</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>9</td>
</tr>
<tr>
<td>69</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>71</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>72</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>73</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>11</td>
</tr>
<tr>
<td>74</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>75</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>76</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>77</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>12</td>
</tr>
<tr>
<td>78</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>79</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>80</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>81</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>9</td>
</tr>
<tr>
<td>82</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>83</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>84</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>85</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>11</td>
</tr>
<tr>
<td>86</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>87</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>88</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>89</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>90</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>9</td>
</tr>
<tr>
<td>91</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>11</td>
</tr>
<tr>
<td>92</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>93</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>16</td>
</tr>
<tr>
<td>94</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>95</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>12</td>
</tr>
<tr>
<td>96</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>97</td>
<td>1</td>
<td>0.75</td>
<td>0.857142857</td>
<td>4</td>
</tr>
<tr>
<td>98</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>99</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>11</td>
</tr>
<tr>
<td>100</td>
<td>1</td>
<td>0.875</td>
<td>0.933333333</td>
<td>8</td>
</tr>
<tr>
<td>101</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>102</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>103</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>104</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>105</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>106</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>107</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>108</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>9</td>
</tr>
<tr>
<td>109</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>110</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>111</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>112</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>113</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>114</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>115</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>116</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>117</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>118</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>119</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>120</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>121</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>9</td>
</tr>
<tr>
<td>122</td>
<td>1</td>
<td>0.833333333</td>
<td>0.909090909</td>
<td>6</td>
</tr>
<tr>
<td>123</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>124</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>125</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>126</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>127</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>128</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>129</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>130</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>131</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>132</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>133</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>15</td>
</tr>
<tr>
<td>134</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>135</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>136</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>13</td>
</tr>
<tr>
<td>137</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>138</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>139</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>140</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>141</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>142</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>143</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>144</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>145</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>9</td>
</tr>
<tr>
<td>146</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>9</td>
</tr>
<tr>
<td>147</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>148</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>30</td>
</tr>
<tr>
<td>149</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>150</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>151</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>11</td>
</tr>
<tr>
<td>152</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>15</td>
</tr>
<tr>
<td>153</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>154</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>155</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>156</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>accuracy</td>
<td>0.993812838</td>
<td>0.993812838</td>
<td>0.993812838</td>
<td>0.993812838</td>
</tr>
<tr>
<td>macro avg</td>
<td>0.999516163</td>
<td>0.990190395</td>
<td>0.994216835</td>
<td>1293</td>
</tr>
<tr>
<td>weighted avg</td>
<td>0.993997625</td>
<td>0.993812838</td>
<td>0.99352785</td>
<td>1293</td>
</tr>
</tbody>
</table>

<p>As expected, once principle component analysis was performed on the features to reduce the dimensionality to N-c first before calculating fisher faces (linear discriminant analysis) with dimensions c-1 (where c is the number of classes), the accuracy of our classifier increased to ~99 %</p>

<h4 id="toc_34">4. Labelled Faces in the Wild (unprocessed)</h4>

<p>Finally, the labelled faces in the wild dataset (unprocessed) images were chosen to perform the same classification using eigenfaces and fisherfaces</p>

<p>For the final case, we set the minimum number of images per class to be: 10 <br>
This results in the following dataset:  <br>
Number of Images: 1500 <br>
Number of Classes: 150 <br>
Features per image: 62500 (250x250) (greater number of features than the previous two datasets!) 
used: <code>cv2.cvtColor(data,cv2.COLOR_BGR2GRAY</code> to convert RGB image to a grey scale image</p>

<h5 id="toc_35">3.1 Sample Images</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20for%20submission/LFW%20sample%20images.png" alt="alt text"></p>

<h5 id="toc_36">3.2 Mean across all faces</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/LFW%20UnprocessedMeans.png" alt="alt text"></p>

<h5 id="toc_37">3.3 First 12 Canonical Basis (Eigen-Faces)</h5>

<p><img src="/Users/azhara001/Documents/MIMS%20UC%20Berkeley/Fall%202022/INFO%20290T/Final%20Project%20Working/Facial%20Recognition%20Project/EigenFaces_LFW_unprocessed.png" alt="alt text"></p>

<h5 id="toc_38">3.4 Classification Using Support Vector Machines (Linear)</h5>

<p>At 88% fall-off for eigen values using eigenfaces</p>

<table>
<thead>
<tr>
<th>classes</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>1.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>2.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>3.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>4.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>5.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>6.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>7.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>8.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>9.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>10.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>11.0</td>
<td>0.250000</td>
<td>0.666667</td>
<td>0.363636</td>
<td>3.000000</td>
</tr>
<tr>
<td>12.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>13.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>14.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>15.0</td>
<td>1.000000</td>
<td>0.200000</td>
<td>0.333333</td>
<td>5.000000</td>
</tr>
<tr>
<td>16.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>17.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>18.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>19.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>20.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>21.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>22.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>23.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>24.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>25.0</td>
<td>0.500000</td>
<td>0.250000</td>
<td>0.333333</td>
<td>4.000000</td>
</tr>
<tr>
<td>26.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>27.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>28.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>29.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>30.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>31.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>32.0</td>
<td>0.500000</td>
<td>1.000000</td>
<td>0.666667</td>
<td>1.000000</td>
</tr>
<tr>
<td>33.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>34.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>35.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>36.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>37.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>38.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>39.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>40.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>41.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>42.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>43.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>44.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>45.0</td>
<td>0.333333</td>
<td>0.250000</td>
<td>0.285714</td>
<td>4.000000</td>
</tr>
<tr>
<td>46.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>47.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>48.0</td>
<td>0.200000</td>
<td>0.500000</td>
<td>0.285714</td>
<td>2.000000</td>
</tr>
<tr>
<td>49.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>50.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>51.0</td>
<td>0.333333</td>
<td>0.200000</td>
<td>0.250000</td>
<td>5.000000</td>
</tr>
<tr>
<td>52.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>53.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>54.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>55.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>56.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>57.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>58.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>59.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>60.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>61.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>62.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>63.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>64.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>65.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>66.0</td>
<td>0.166667</td>
<td>0.333333</td>
<td>0.222222</td>
<td>3.000000</td>
</tr>
<tr>
<td>67.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>68.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>69.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>70.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>71.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>72.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>73.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>74.0</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>75.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>76.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>77.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>78.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>79.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>80.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>81.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>82.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>83.0</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>84.0</td>
<td>1.000000</td>
<td>0.250000</td>
<td>0.400000</td>
<td>4.000000</td>
</tr>
<tr>
<td>85.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>86.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>87.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>88.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>89.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>90.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>91.0</td>
<td>0.333333</td>
<td>0.250000</td>
<td>0.285714</td>
<td>4.000000</td>
</tr>
<tr>
<td>92.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>93.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>8.000000</td>
</tr>
<tr>
<td>94.0</td>
<td>0.125000</td>
<td>0.500000</td>
<td>0.200000</td>
<td>2.000000</td>
</tr>
<tr>
<td>95.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>96.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>97.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>98.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>99.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>100.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>101.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>102.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>103.0</td>
<td>0.250000</td>
<td>0.333333</td>
<td>0.285714</td>
<td>3.000000</td>
</tr>
<tr>
<td>104.0</td>
<td>0.333333</td>
<td>0.500000</td>
<td>0.400000</td>
<td>2.000000</td>
</tr>
<tr>
<td>105.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>106.0</td>
<td>0.111111</td>
<td>0.333333</td>
<td>0.166667</td>
<td>3.000000</td>
</tr>
<tr>
<td>107.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>108.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>109.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>110.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>112.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>113.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>114.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>115.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>116.0</td>
<td>0.200000</td>
<td>0.333333</td>
<td>0.250000</td>
<td>3.000000</td>
</tr>
<tr>
<td>117.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>118.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>119.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>120.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>121.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>122.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>123.0</td>
<td>0.200000</td>
<td>0.250000</td>
<td>0.222222</td>
<td>4.000000</td>
</tr>
<tr>
<td>124.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>125.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>127.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>128.0</td>
<td>0.333333</td>
<td>0.250000</td>
<td>0.285714</td>
<td>4.000000</td>
</tr>
<tr>
<td>129.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>130.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>131.0</td>
<td>0.250000</td>
<td>0.500000</td>
<td>0.333333</td>
<td>2.000000</td>
</tr>
<tr>
<td>132.0</td>
<td>1.000000</td>
<td>0.200000</td>
<td>0.333333</td>
<td>5.000000</td>
</tr>
<tr>
<td>133.0</td>
<td>0.750000</td>
<td>0.750000</td>
<td>0.750000</td>
<td>4.000000</td>
</tr>
<tr>
<td>134.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>135.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>136.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>137.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>138.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>139.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>140.0</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>141.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>142.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>143.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>144.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>145.0</td>
<td>0.125000</td>
<td>0.250000</td>
<td>0.166667</td>
<td>4.000000</td>
</tr>
<tr>
<td>146.0</td>
<td>1.000000</td>
<td>0.200000</td>
<td>0.333333</td>
<td>5.000000</td>
</tr>
<tr>
<td>147.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>148.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>149.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>accuracy</td>
<td>0.062222</td>
<td>0.062222</td>
<td>0.062222</td>
<td>0.062222</td>
</tr>
<tr>
<td>macroavg</td>
<td>0.083071</td>
<td>0.062838</td>
<td>0.058468</td>
<td>450.000000</td>
</tr>
<tr>
<td>weightedavg</td>
<td>0.100481</td>
<td>0.062222</td>
<td>0.063638</td>
<td>450.000000</td>
</tr>
</tbody>
</table>

<h5 id="toc_39">3.5 Classification Using Support Vector Machines (Linear) for Fisher Faces <br></h5>

<p>Using 88% fall off eigen vectors</p>

<table>
<thead>
<tr>
<th>Classes</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0.0</td>
<td>1.000000</td>
<td>0.250000</td>
<td>0.400000</td>
<td>4.000000</td>
</tr>
<tr>
<td>1.0</td>
<td>0.250000</td>
<td>0.500000</td>
<td>0.333333</td>
<td>2.000000</td>
</tr>
<tr>
<td>2.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>3.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>4.0</td>
<td>0.250000</td>
<td>0.250000</td>
<td>0.250000</td>
<td>4.000000</td>
</tr>
<tr>
<td>5.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>6.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>7.0</td>
<td>0.500000</td>
<td>0.500000</td>
<td>0.500000</td>
<td>4.000000</td>
</tr>
<tr>
<td>8.0</td>
<td>0.166667</td>
<td>1.000000</td>
<td>0.285714</td>
<td>2.000000</td>
</tr>
<tr>
<td>9.0</td>
<td>0.200000</td>
<td>0.500000</td>
<td>0.285714</td>
<td>2.000000</td>
</tr>
<tr>
<td>10.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>11.0</td>
<td>0.666667</td>
<td>0.666667</td>
<td>0.666667</td>
<td>3.000000</td>
</tr>
<tr>
<td>12.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>13.0</td>
<td>0.333333</td>
<td>0.333333</td>
<td>0.333333</td>
<td>3.000000</td>
</tr>
<tr>
<td>14.0</td>
<td>0.500000</td>
<td>0.333333</td>
<td>0.400000</td>
<td>3.000000</td>
</tr>
<tr>
<td>15.0</td>
<td>1.000000</td>
<td>0.600000</td>
<td>0.750000</td>
<td>5.000000</td>
</tr>
<tr>
<td>16.0</td>
<td>0.666667</td>
<td>0.666667</td>
<td>0.666667</td>
<td>3.000000</td>
</tr>
<tr>
<td>17.0</td>
<td>0.333333</td>
<td>0.500000</td>
<td>0.400000</td>
<td>2.000000</td>
</tr>
<tr>
<td>18.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>19.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>20.0</td>
<td>1.000000</td>
<td>0.250000</td>
<td>0.400000</td>
<td>4.000000</td>
</tr>
<tr>
<td>21.0</td>
<td>1.000000</td>
<td>0.500000</td>
<td>0.666667</td>
<td>4.000000</td>
</tr>
<tr>
<td>22.0</td>
<td>0.200000</td>
<td>0.333333</td>
<td>0.250000</td>
<td>3.000000</td>
</tr>
<tr>
<td>24.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>25.0</td>
<td>0.333333</td>
<td>0.250000</td>
<td>0.285714</td>
<td>4.000000</td>
</tr>
<tr>
<td>26.0</td>
<td>1.000000</td>
<td>0.500000</td>
<td>0.666667</td>
<td>2.000000</td>
</tr>
<tr>
<td>27.0</td>
<td>0.500000</td>
<td>0.333333</td>
<td>0.400000</td>
<td>3.000000</td>
</tr>
<tr>
<td>28.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>29.0</td>
<td>0.500000</td>
<td>0.200000</td>
<td>0.285714</td>
<td>5.000000</td>
</tr>
<tr>
<td>30.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>31.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>32.0</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>33.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>34.0</td>
<td>0.333333</td>
<td>0.500000</td>
<td>0.400000</td>
<td>4.000000</td>
</tr>
<tr>
<td>35.0</td>
<td>1.000000</td>
<td>0.200000</td>
<td>0.333333</td>
<td>5.000000</td>
</tr>
<tr>
<td>36.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>37.0</td>
<td>0.200000</td>
<td>0.200000</td>
<td>0.200000</td>
<td>5.000000</td>
</tr>
<tr>
<td>38.0</td>
<td>0.500000</td>
<td>0.500000</td>
<td>0.500000</td>
<td>4.000000</td>
</tr>
<tr>
<td>39.0</td>
<td>0.500000</td>
<td>0.333333</td>
<td>0.400000</td>
<td>3.000000</td>
</tr>
<tr>
<td>40.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>41.0</td>
<td>0.250000</td>
<td>0.250000</td>
<td>0.250000</td>
<td>4.000000</td>
</tr>
<tr>
<td>42.0</td>
<td>0.200000</td>
<td>0.250000</td>
<td>0.222222</td>
<td>4.000000</td>
</tr>
<tr>
<td>43.0</td>
<td>0.200000</td>
<td>0.333333</td>
<td>0.250000</td>
<td>3.000000</td>
</tr>
<tr>
<td>44.0</td>
<td>1.000000</td>
<td>0.666667</td>
<td>0.800000</td>
<td>3.000000</td>
</tr>
<tr>
<td>45.0</td>
<td>0.166667</td>
<td>0.250000</td>
<td>0.200000</td>
<td>4.000000</td>
</tr>
<tr>
<td>46.0</td>
<td>1.000000</td>
<td>0.250000</td>
<td>0.400000</td>
<td>4.000000</td>
</tr>
<tr>
<td>47.0</td>
<td>0.166667</td>
<td>1.000000</td>
<td>0.285714</td>
<td>1.000000</td>
</tr>
<tr>
<td>48.0</td>
<td>0.250000</td>
<td>0.500000</td>
<td>0.333333</td>
<td>2.000000</td>
</tr>
<tr>
<td>49.0</td>
<td>0.250000</td>
<td>0.333333</td>
<td>0.285714</td>
<td>3.000000</td>
</tr>
<tr>
<td>50.0</td>
<td>1.000000</td>
<td>0.400000</td>
<td>0.571429</td>
<td>5.000000</td>
</tr>
<tr>
<td>51.0</td>
<td>0.333333</td>
<td>0.200000</td>
<td>0.250000</td>
<td>5.000000</td>
</tr>
<tr>
<td>52.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>53.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>54.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>55.0</td>
<td>0.400000</td>
<td>0.500000</td>
<td>0.444444</td>
<td>4.000000</td>
</tr>
<tr>
<td>56.0</td>
<td>0.500000</td>
<td>0.500000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>57.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>58.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>59.0</td>
<td>0.500000</td>
<td>0.200000</td>
<td>0.285714</td>
<td>5.000000</td>
</tr>
<tr>
<td>60.0</td>
<td>0.500000</td>
<td>0.333333</td>
<td>0.400000</td>
<td>3.000000</td>
</tr>
<tr>
<td>61.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>62.0</td>
<td>0.333333</td>
<td>0.333333</td>
<td>0.333333</td>
<td>3.000000</td>
</tr>
<tr>
<td>63.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>64.0</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>65.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>66.0</td>
<td>0.500000</td>
<td>0.666667</td>
<td>0.571429</td>
<td>3.000000</td>
</tr>
<tr>
<td>67.0</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>68.0</td>
<td>0.250000</td>
<td>0.250000</td>
<td>0.250000</td>
<td>4.000000</td>
</tr>
<tr>
<td>69.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>70.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>71.0</td>
<td>1.000000</td>
<td>0.250000</td>
<td>0.400000</td>
<td>4.000000</td>
</tr>
<tr>
<td>72.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>73.0</td>
<td>1.000000</td>
<td>0.250000</td>
<td>0.400000</td>
<td>4.000000</td>
</tr>
<tr>
<td>74.0</td>
<td>0.200000</td>
<td>0.333333</td>
<td>0.250000</td>
<td>3.000000</td>
</tr>
<tr>
<td>75.0</td>
<td>0.333333</td>
<td>0.200000</td>
<td>0.250000</td>
<td>5.000000</td>
</tr>
<tr>
<td>76.0</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>77.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>78.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>79.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>80.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>81.0</td>
<td>1.000000</td>
<td>0.500000</td>
<td>0.666667</td>
<td>2.000000</td>
</tr>
<tr>
<td>82.0</td>
<td>0.333333</td>
<td>0.500000</td>
<td>0.400000</td>
<td>2.000000</td>
</tr>
<tr>
<td>83.0</td>
<td>0.285714</td>
<td>0.666667</td>
<td>0.400000</td>
<td>3.000000</td>
</tr>
<tr>
<td>84.0</td>
<td>0.500000</td>
<td>0.250000</td>
<td>0.333333</td>
<td>4.000000</td>
</tr>
<tr>
<td>85.0</td>
<td>1.000000</td>
<td>0.500000</td>
<td>0.666667</td>
<td>2.000000</td>
</tr>
<tr>
<td>86.0</td>
<td>0.250000</td>
<td>0.250000</td>
<td>0.250000</td>
<td>4.000000</td>
</tr>
<tr>
<td>87.0</td>
<td>0.600000</td>
<td>0.600000</td>
<td>0.600000</td>
<td>5.000000</td>
</tr>
<tr>
<td>88.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>89.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>90.0</td>
<td>0.250000</td>
<td>0.250000</td>
<td>0.250000</td>
<td>4.000000</td>
</tr>
<tr>
<td>91.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>92.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>93.0</td>
<td>1.000000</td>
<td>0.250000</td>
<td>0.400000</td>
<td>8.000000</td>
</tr>
<tr>
<td>94.0</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>95.0</td>
<td>0.333333</td>
<td>0.333333</td>
<td>0.333333</td>
<td>3.000000</td>
</tr>
<tr>
<td>96.0</td>
<td>0.500000</td>
<td>0.500000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>97.0</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>98.0</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>6.000000</td>
</tr>
<tr>
<td>99.0</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>100.0</td>
<td>0.666667</td>
<td>0.666667</td>
<td>0.666667</td>
<td>3.000000</td>
</tr>
<tr>
<td>101.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>102.0</td>
<td>0.666667</td>
<td>0.666667</td>
<td>0.666667</td>
<td>3.000000</td>
</tr>
<tr>
<td>103.0</td>
<td>0.333333</td>
<td>0.333333</td>
<td>0.333333</td>
<td>3.000000</td>
</tr>
<tr>
<td>104.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>105.0</td>
<td>0.750000</td>
<td>0.600000</td>
<td>0.666667</td>
<td>5.000000</td>
</tr>
<tr>
<td>106.0</td>
<td>1.000000</td>
<td>0.666667</td>
<td>0.800000</td>
<td>3.000000</td>
</tr>
<tr>
<td>107.0</td>
<td>0.250000</td>
<td>1.000000</td>
<td>0.400000</td>
<td>1.000000</td>
</tr>
<tr>
<td>108.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>109.0</td>
<td>0.500000</td>
<td>0.500000</td>
<td>0.500000</td>
<td>4.000000</td>
</tr>
<tr>
<td>110.0</td>
<td>0.250000</td>
<td>0.333333</td>
<td>0.285714</td>
<td>3.000000</td>
</tr>
<tr>
<td>111.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>112.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>113.0</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>6.000000</td>
</tr>
<tr>
<td>114.0</td>
<td>0.200000</td>
<td>1.000000</td>
<td>0.333333</td>
<td>1.000000</td>
</tr>
<tr>
<td>115.0</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>116.0</td>
<td>0.428571</td>
<td>1.000000</td>
<td>0.600000</td>
<td>3.000000</td>
</tr>
<tr>
<td>117.0</td>
<td>0.285714</td>
<td>0.666667</td>
<td>0.400000</td>
<td>3.000000</td>
</tr>
<tr>
<td>118.0</td>
<td>1.000000</td>
<td>0.250000</td>
<td>0.400000</td>
<td>4.000000</td>
</tr>
<tr>
<td>119.0</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>120.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>121.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>122.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>123.0</td>
<td>0.333333</td>
<td>0.250000</td>
<td>0.285714</td>
<td>4.000000</td>
</tr>
<tr>
<td>124.0</td>
<td>0.333333</td>
<td>0.333333</td>
<td>0.333333</td>
<td>3.000000</td>
</tr>
<tr>
<td>125.0</td>
<td>0.333333</td>
<td>1.000000</td>
<td>0.500000</td>
<td>1.000000</td>
</tr>
<tr>
<td>127.0</td>
<td>1.000000</td>
<td>0.666667</td>
<td>0.800000</td>
<td>3.000000</td>
</tr>
<tr>
<td>128.0</td>
<td>1.000000</td>
<td>0.750000</td>
<td>0.857143</td>
<td>4.000000</td>
</tr>
<tr>
<td>129.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>130.0</td>
<td>0.600000</td>
<td>1.000000</td>
<td>0.750000</td>
<td>3.000000</td>
</tr>
<tr>
<td>131.0</td>
<td>0.400000</td>
<td>1.000000</td>
<td>0.571429</td>
<td>2.000000</td>
</tr>
<tr>
<td>132.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>133.0</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>134.0</td>
<td>0.333333</td>
<td>0.500000</td>
<td>0.400000</td>
<td>2.000000</td>
</tr>
<tr>
<td>135.0</td>
<td>1.000000</td>
<td>0.166667</td>
<td>0.285714</td>
<td>6.000000</td>
</tr>
<tr>
<td>136.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>137.0</td>
<td>0.333333</td>
<td>0.500000</td>
<td>0.400000</td>
<td>2.000000</td>
</tr>
<tr>
<td>138.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>139.0</td>
<td>0.333333</td>
<td>0.500000</td>
<td>0.400000</td>
<td>2.000000</td>
</tr>
<tr>
<td>140.0</td>
<td>0.500000</td>
<td>0.333333</td>
<td>0.400000</td>
<td>3.000000</td>
</tr>
<tr>
<td>141.0</td>
<td>0.250000</td>
<td>0.250000</td>
<td>0.250000</td>
<td>4.000000</td>
</tr>
<tr>
<td>142.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>143.0</td>
<td>0.333333</td>
<td>0.500000</td>
<td>0.400000</td>
<td>2.000000</td>
</tr>
<tr>
<td>144.0</td>
<td>0.666667</td>
<td>0.400000</td>
<td>0.500000</td>
<td>5.000000</td>
</tr>
<tr>
<td>145.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>146.0</td>
<td>1.000000</td>
<td>0.600000</td>
<td>0.750000</td>
<td>5.000000</td>
</tr>
<tr>
<td>147.0</td>
<td>0.250000</td>
<td>1.000000</td>
<td>0.400000</td>
<td>1.000000</td>
</tr>
<tr>
<td>148.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>149.0</td>
<td>1.000000</td>
<td>0.750000</td>
<td>0.857143</td>
<td>4.000000</td>
</tr>
<tr>
<td>accuracy</td>
<td>0.322222</td>
<td>0.322222</td>
<td>0.322222</td>
<td>0.322222</td>
</tr>
<tr>
<td>macroavg</td>
<td>0.389527</td>
<td>0.332658</td>
<td>0.317471</td>
<td>450.000000</td>
</tr>
<tr>
<td>weightedavg</td>
<td>0.449593</td>
<td>0.322222</td>
<td>0.336899</td>
<td>450.000000</td>
</tr>
</tbody>
</table>

<h5 id="toc_40">3.6 Classification using eigenfaces (N-c) dimensionality for support vector machines</h5>

<table>
<thead>
<tr>
<th>Classes</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>1.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>2.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>3.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>4.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>5.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>6.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>7.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>8.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>9.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>10.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>11.0</td>
<td>0.250000</td>
<td>0.666667</td>
<td>0.363636</td>
<td>3.000000</td>
</tr>
<tr>
<td>12.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>13.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>14.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>15.0</td>
<td>1.000000</td>
<td>0.200000</td>
<td>0.333333</td>
<td>5.000000</td>
</tr>
<tr>
<td>16.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>17.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>18.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>19.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>20.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>21.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>22.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>23.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>24.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>25.0</td>
<td>0.500000</td>
<td>0.250000</td>
<td>0.333333</td>
<td>4.000000</td>
</tr>
<tr>
<td>26.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>27.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>28.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>29.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>30.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>31.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>32.0</td>
<td>0.500000</td>
<td>1.000000</td>
<td>0.666667</td>
<td>1.000000</td>
</tr>
<tr>
<td>33.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>34.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>35.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>36.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>37.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>38.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>39.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>40.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>41.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>42.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>43.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>44.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>45.0</td>
<td>0.333333</td>
<td>0.250000</td>
<td>0.285714</td>
<td>4.000000</td>
</tr>
<tr>
<td>46.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>47.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>48.0</td>
<td>0.200000</td>
<td>0.500000</td>
<td>0.285714</td>
<td>2.000000</td>
</tr>
<tr>
<td>49.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>50.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>51.0</td>
<td>0.333333</td>
<td>0.200000</td>
<td>0.250000</td>
<td>5.000000</td>
</tr>
<tr>
<td>52.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>53.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>54.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>55.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>56.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>57.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>58.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>59.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>60.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>61.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>62.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>63.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>64.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>65.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>66.0</td>
<td>0.166667</td>
<td>0.333333</td>
<td>0.222222</td>
<td>3.000000</td>
</tr>
<tr>
<td>67.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>68.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>69.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>70.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>71.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>72.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>73.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>74.0</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>75.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>76.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>77.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>78.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>79.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>80.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>81.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>82.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>83.0</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>84.0</td>
<td>1.000000</td>
<td>0.250000</td>
<td>0.400000</td>
<td>4.000000</td>
</tr>
<tr>
<td>85.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>86.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>87.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>88.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>89.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>90.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>91.0</td>
<td>0.333333</td>
<td>0.250000</td>
<td>0.285714</td>
<td>4.000000</td>
</tr>
<tr>
<td>92.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>93.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>8.000000</td>
</tr>
<tr>
<td>94.0</td>
<td>0.125000</td>
<td>0.500000</td>
<td>0.200000</td>
<td>2.000000</td>
</tr>
<tr>
<td>95.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>96.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>97.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>98.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>99.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>100.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>101.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>102.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>103.0</td>
<td>0.250000</td>
<td>0.333333</td>
<td>0.285714</td>
<td>3.000000</td>
</tr>
<tr>
<td>104.0</td>
<td>0.333333</td>
<td>0.500000</td>
<td>0.400000</td>
<td>2.000000</td>
</tr>
<tr>
<td>105.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>106.0</td>
<td>0.111111</td>
<td>0.333333</td>
<td>0.166667</td>
<td>3.000000</td>
</tr>
<tr>
<td>107.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>108.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>109.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>110.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>112.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>113.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>114.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>115.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>116.0</td>
<td>0.200000</td>
<td>0.333333</td>
<td>0.250000</td>
<td>3.000000</td>
</tr>
<tr>
<td>117.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>118.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>119.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>120.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>121.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>122.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>123.0</td>
<td>0.200000</td>
<td>0.250000</td>
<td>0.222222</td>
<td>4.000000</td>
</tr>
<tr>
<td>124.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>125.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>127.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>128.0</td>
<td>0.333333</td>
<td>0.250000</td>
<td>0.285714</td>
<td>4.000000</td>
</tr>
<tr>
<td>129.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>130.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>131.0</td>
<td>0.250000</td>
<td>0.500000</td>
<td>0.333333</td>
<td>2.000000</td>
</tr>
<tr>
<td>132.0</td>
<td>1.000000</td>
<td>0.200000</td>
<td>0.333333</td>
<td>5.000000</td>
</tr>
<tr>
<td>133.0</td>
<td>0.750000</td>
<td>0.750000</td>
<td>0.750000</td>
<td>4.000000</td>
</tr>
<tr>
<td>134.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>135.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>136.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>137.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>138.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>139.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>140.0</td>
<td>1.000000</td>
<td>0.333333</td>
<td>0.500000</td>
<td>3.000000</td>
</tr>
<tr>
<td>141.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>142.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>143.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>144.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>145.0</td>
<td>0.125000</td>
<td>0.250000</td>
<td>0.166667</td>
<td>4.000000</td>
</tr>
<tr>
<td>146.0</td>
<td>1.000000</td>
<td>0.200000</td>
<td>0.333333</td>
<td>5.000000</td>
</tr>
<tr>
<td>147.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>148.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>149.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>accuracy</td>
<td>0.062222</td>
<td>0.062222</td>
<td>0.062222</td>
<td>0.062222</td>
</tr>
<tr>
<td>macro avg</td>
<td>0.083071</td>
<td>0.062838</td>
<td>0.058468</td>
<td>450.000000</td>
</tr>
<tr>
<td>weighted avg</td>
<td>0.100481</td>
<td>0.062222</td>
<td>0.063638</td>
<td>450.000000</td>
</tr>
</tbody>
</table>

<p>Notice how the accuracy for eigenfaces using 88% fall off versus N-c features does not change by much. This is intuitive since we are capturing majority of the features using the 88% fall-off</p>

<h5 id="toc_41">3.7 Classification using fisherfaces (c-1) dimensionality for support vector machines</h5>

<p>However, once C-1 features were extracted for fisherfaces and fed into the SVM, the results were remarkably interesting:</p>

<table>
<thead>
<tr>
<th>Classes</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>2.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>4.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>5.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>6.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>7.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>8.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>9.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>10.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>11.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>12.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>13.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>14.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>15.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>5.0</td>
</tr>
<tr>
<td>16.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>17.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>18.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>19.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>6.0</td>
</tr>
<tr>
<td>20.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>21.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>22.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>24.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>25.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>26.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>27.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>28.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>29.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>5.0</td>
</tr>
<tr>
<td>30.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>31.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>32.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>33.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>34.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>35.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>5.0</td>
</tr>
<tr>
<td>36.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>37.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>5.0</td>
</tr>
<tr>
<td>38.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>39.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>40.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>6.0</td>
</tr>
<tr>
<td>41.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>42.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>43.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>44.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>45.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>46.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>47.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>48.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>49.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>50.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>5.0</td>
</tr>
<tr>
<td>51.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>5.0</td>
</tr>
<tr>
<td>52.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>53.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>54.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>55.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>56.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>57.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>58.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>59.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>5.0</td>
</tr>
<tr>
<td>60.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>61.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>62.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>63.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>64.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>65.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>66.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>67.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>68.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>69.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>70.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>71.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>72.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>73.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>74.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>75.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>5.0</td>
</tr>
<tr>
<td>76.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>77.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>78.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>79.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>80.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>81.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>82.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>83.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>84.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>85.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>86.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>87.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>5.0</td>
</tr>
<tr>
<td>88.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>89.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>90.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>91.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>92.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>93.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>8.0</td>
</tr>
<tr>
<td>94.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>95.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>96.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>97.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>98.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>6.0</td>
</tr>
<tr>
<td>99.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>100.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>101.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>102.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>103.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>104.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>105.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>5.0</td>
</tr>
<tr>
<td>106.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>107.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>108.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>109.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>110.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>112.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>113.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>6.0</td>
</tr>
<tr>
<td>114.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>115.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>116.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>117.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>118.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>119.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>120.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>122.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>123.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>124.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>125.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>127.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>128.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>129.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>130.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>131.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>132.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>5.0</td>
</tr>
<tr>
<td>133.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>134.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>135.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>6.0</td>
</tr>
<tr>
<td>137.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>138.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>139.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>140.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>3.0</td>
</tr>
<tr>
<td>141.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>142.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>143.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<td>144.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>5.0</td>
</tr>
<tr>
<td>145.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>146.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>5.0</td>
</tr>
<tr>
<td>147.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>148.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>5.0</td>
</tr>
<tr>
<td>149.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>4.0</td>
</tr>
<tr>
<td>accuracy</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
<tr>
<td>macroavg</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>450.0</td>
</tr>
<tr>
<td>weightedavg</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
<td>450.0</td>
</tr>
</tbody>
</table>

<p>This result shows that once c-1 features were computed for the fisherfaces before being fed into te SVM classifier, the SVM does a perfect job in classifying all of the images for each class. </p>

<p>Finally, in one last attempt to confuse the classifier, all the labels for the test set were randomized to observe if we get a near zero accuracy score</p>

<h5 id="toc_42">3.8 Confusing the classifier by shuffling labels</h5>

<p>Output of fisherfaces classification following SVM:</p>

<table>
<thead>
<tr>
<th>Classes</th>
<th>precision</th>
<th>recall</th>
<th>f1-score</th>
<th>support</th>
</tr>
</thead>

<tbody>
<tr>
<td>0.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>1.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>2.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>4.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>5.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>6.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>7.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>8.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>9.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>10.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>11.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>12.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>13.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>14.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>15.0</td>
<td>0.200000</td>
<td>0.200000</td>
<td>0.200000</td>
<td>5.000000</td>
</tr>
<tr>
<td>16.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>17.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>18.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>19.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>20.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>21.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>22.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>24.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>25.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>26.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>27.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>28.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>29.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>30.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>31.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>32.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>33.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>34.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>35.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>36.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>37.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>38.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>39.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>40.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>41.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>42.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>43.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>44.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>45.0</td>
<td>0.250000</td>
<td>0.250000</td>
<td>0.250000</td>
<td>4.000000</td>
</tr>
<tr>
<td>46.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>47.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>48.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>49.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>50.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>51.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>52.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>53.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>54.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>55.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>56.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>57.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>58.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>59.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>60.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>61.0</td>
<td>0.250000</td>
<td>0.250000</td>
<td>0.250000</td>
<td>4.000000</td>
</tr>
<tr>
<td>62.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>63.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>64.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>65.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>66.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>67.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>68.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>69.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>70.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>71.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>72.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>73.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>74.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>75.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>76.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>77.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>78.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>79.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>80.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>81.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>82.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>83.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>84.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>85.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>86.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>87.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>88.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>89.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>90.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>91.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>92.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>93.0</td>
<td>0.125000</td>
<td>0.125000</td>
<td>0.125000</td>
<td>8.000000</td>
</tr>
<tr>
<td>94.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>95.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>96.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>97.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>98.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>99.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>100.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>101.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>102.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>103.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>104.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>105.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>106.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>107.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>108.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>109.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>110.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>112.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>113.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.000000</td>
</tr>
<tr>
<td>114.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>115.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>116.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>117.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>118.0</td>
<td>0.250000</td>
<td>0.250000</td>
<td>0.250000</td>
<td>4.000000</td>
</tr>
<tr>
<td>119.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>120.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>122.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>123.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>124.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>125.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>127.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>128.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>129.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>130.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>131.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>132.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>133.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>134.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>135.0</td>
<td>0.166667</td>
<td>0.166667</td>
<td>0.166667</td>
<td>6.000000</td>
</tr>
<tr>
<td>137.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>138.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>139.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>140.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>3.000000</td>
</tr>
<tr>
<td>141.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>142.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>143.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>2.000000</td>
</tr>
<tr>
<td>144.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>145.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>146.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>147.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>148.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.000000</td>
</tr>
<tr>
<td>149.0</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>4.000000</td>
</tr>
<tr>
<td>accuracy</td>
<td>0.013333</td>
<td>0.013333</td>
<td>0.013333</td>
<td>0.013333</td>
</tr>
<tr>
<td>macroavg</td>
<td>0.008623</td>
<td>0.008623</td>
<td>0.008623</td>
<td>450.000000</td>
</tr>
<tr>
<td>weightedavg</td>
<td>0.013333</td>
<td>0.013333</td>
<td>0.013333</td>
<td>450.000000</td>
</tr>
</tbody>
</table>

<p>The low accuracy, as expected results in a low accuracy score which makes intuitive sense for this case</p>

<h5 id="toc_43">3.9 Potential limitations and future recommendations</h5>

<p><strong>While eigenfaces serves as a principle component analysis technique to reduce the dimensionality of features before being fed into a support vector machine, another method as mentioned by Peter N. Belhumeur, Joao P Hespanha, David J. Kriegman, 1997/7, uses linear discriminant analysis that minimizes the within class scatter and maximizes the between class scatter followed by projection onto c-1 features where c is the number of classes. For this project, however, a unique approach was taken such that: <br>
1. PCA was performed to reduce the number of features to N-c where N is the number of images and c is the number of classes <br>
2. This reduced dimensional data was then fed into the fisherfaces linear discriminant analysis code to reduce the dimensionality to c-1 <br>
3. Then, the reduced features of the fisherfaces were fed into another classifier, the linear suppor vector machines, which churned out remarkably accurate results <br></strong></p>

<p><strong>Going forward, a more raw dataset should be tested on this algorithm with more datasets and number of classes. Owing to the computational requirements for training an SVM on thousands of images, it would be interesting to follow the same methodology on neural networks</strong></p>




</body>

</html>
